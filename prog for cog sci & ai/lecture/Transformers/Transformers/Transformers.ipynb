{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are state-of-the-art for processing sequences. Their first successes were with text processing and machine translation in particular.\n",
    "\n",
    "Unlike RNNs, LSTMs and GRUs there are no recurrent connections so there is no memory of the previous state. The entire sequence is input.\n",
    "\n",
    "**Attention** overcomes the lack of memory by perceiving the entire sequence at once, enabling long-range semantic dependencies but at a cost of computer memory.\n",
    "\n",
    "Since there is no sequential processing, transformers are suited to parallel hardware environments \n",
    "\n",
    "The transformer model follows the same general pattern as a standard sequence to sequence with attention model:\n",
    "\n",
    "- The input sentence is passed through N encoder layers that generates an output for each word/token in the sequence.\n",
    "\n",
    "- The decoder attends on the encoder's output and its own input (self-attention) to predict the next word.\n",
    "\n",
    "\n",
    "![](Transformer.png)\n",
    "\n",
    "$\\text{Figure 1. Basic Transformer Architecture, Left: Encoder, Right: Decoder}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Basic Components for Machine Translation\n",
    "\n",
    "#### Input Encoder\n",
    "\n",
    "Input Embedding: The embedding for the source sentence (from the source language).\n",
    "\n",
    "Positional Encoding: Serial positions of the words in the source sentence.\n",
    "\n",
    "Multi-Head Attention: Attention over the source sentence.\n",
    "\n",
    "Feed-Forward Network: Computes non-linear hierarchical features. Output goes to Multi-head attention in the decoder.\n",
    "\n",
    "#### Output Decoder\n",
    "\n",
    "Embedding: The embedding for the target sentence (from the target language). \n",
    "\n",
    "Positional Encoding: Serial positions of the words generated so far.\n",
    "\n",
    "Masked Multi-Head Attention: Attention over the words generated so far.\n",
    "\n",
    "Multi-Head Attention from Input and Output: Query from the output, keys and values from the input.\n",
    "\n",
    "Feed-Forward Network: Computes non-linear hierarchical features\n",
    "\n",
    "Final Linear Layer with Softmax: Probabilities for next word\n",
    "\n",
    "Note: $N_x$ is the number of encoder or decoder blocks to stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    "\n",
    "Attention is a parallel operation not sequential so word position is lost. Positional Encoding adds word position information.\n",
    "\n",
    "Different sine and cosine frequencies indicate relative position of words.\n",
    "\n",
    "Let:\n",
    "t = position of word\n",
    "d = embedding dimension\n",
    "pe = positional encoding vector, its length = d\n",
    "\n",
    "Then:\n",
    "\n",
    "$$pe(t,2i) = sin\\left(\\frac{t}{10000^\\frac{2i}{d}}\\right) \\\\\n",
    "pe(t,2i+1) = cos\\left(\\frac{t}{10000^\\frac{2i}{d}}\\right) $$\n",
    "\n",
    "![](positional_encoding.png)\n",
    "\n",
    "Above depicts the positional encoding for an encoding dimension d = 128. The rows are the positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "Ashish Vaswami,et.al.(2017)Attention Is All You Need. https://arxiv.org/abs/1706.03762\n",
    "\n",
    "Attention is a way to capture dependencies among in sequences without sequential recurrent connections. The entire sequence processed in one step. This eliminates back-propagation through time.\n",
    "\n",
    "Intuitively think of attention as a weighted average of past elements of a sequence.\n",
    "\n",
    "#### Basic Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Attention(Q,K,V) = Softmax_k\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](BasicAttention.png)\n",
    "\n",
    "$\\text{Figure2. Basic Attention}$\n",
    "\n",
    "As a word is being processed we want information from all the other words in the sequence. Relationship of input word to all the other words in parallel.\n",
    "\n",
    "Key-value store paradigm: Information Retreival Systems, Dictionary in Python\n",
    "- Q Weight matrix representing a Query (i.e. What information in the rest of the sequence do I want for this word)\n",
    "- K Weight matrix representing a Key (i.e. something like a label, e.g. color)\n",
    "- V Weight matrix representing a Value, (i.e. the value of the label e.g. red)\n",
    "    \n",
    "Think of these matrices as rotating the input vector in the embedding vector space.\n",
    "\n",
    "The key and the query must be the same dimension.\n",
    "\n",
    "The output of attention is a weighted (i.e. probability) Value vector. (How should attention be allocated in the sequence).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "![](Attention2.png)\n",
    "\n",
    "$\\text{Figure 3. Multi-Head Attention}$\n",
    "\n",
    "Multiple attention (i.e. multiple hidden layers in Figure 3.) block in parallel to attend to different attributes. (e.g. color, gender, location)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add & Norm\n",
    "\n",
    "ADD: adds residual connections to prevent vanishing gradients.   \n",
    "Norm: Batch Normalization to stabilize the computations.\n",
    "\n",
    "#### Linear Layer\n",
    "\n",
    "A regular linear layer computing $y = WX^T +b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Systems Based on Attention-Transformer Architecture\n",
    "\n",
    "#### BERT - Bidirectional Encoder Representation from Transformers\n",
    "\n",
    "Google AI Language: https://arxiv.org/pdf/1810.04805.pdf (2019)\n",
    "\n",
    "BERT stacks Encoders to attend over both left and right context (i.e. Bidirectional)\n",
    "BERT Pre-trained:\n",
    "\n",
    "- Masked Language Model: fill in words that are masked out\n",
    "- Next Sentence Prediction: does second sentence follow from first\n",
    "- Fine Tuning: must fine tune for each new language task\n",
    "\n",
    "BERT has 110 million parameters\n",
    "\n",
    "#### GPT-2, GPT-3 : Generative Pre-Training\n",
    "\n",
    "Open AI  \n",
    "GPT-2:  https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf (2019)  \n",
    "GPT-3: https://arxiv.org/pdf/2005.14165.pdf (2020)  \n",
    "\n",
    "GPT-3 stacks Transformer Decoders to perform one-shot learning to do all language tasks. Don't need fine tuning for separate language tasks.\n",
    "\n",
    "GPT-2 has 1.5 Billion parameters, GPT-3 has 175 Billion parameters.  GPT-3 is the same architecture as GPT-2 only much much bigger. GPT-3 trained with Common Crawl (i.e. the entire Internet)  \n",
    "\n",
    "GPT-2 online demo https://gpt2.ai-demo.xyz/\n",
    "\n",
    "OpenAI has recently opened up the GPT-3 API https://openai.com/api/\n",
    "\n",
    "GPT-3 is much better than GPT-2 because the scale (training corpus and number of parameters) is so much bigger.\n",
    "\n",
    "A lot of hype about GPT-3!!! It appears to generate some realistic text but also can generate silly things\n",
    "\n",
    "https://machinelearningknowledge.ai/openai-gpt-3-demos-to-convince-you-that-ai-threat-is-real-or-is-it/\n",
    "\n",
    "### The Problem with Language Models\n",
    "\n",
    "**No Real Language Understanding**. **Language Understanding requires experience not just processing correlations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Ashish Vaswami, et.al. (2017) Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "https://www.kdnuggets.com/2020/08/transformer-architecture-development-transformer-models.html\n",
    "\n",
    "https://blog.exxactcorp.com/a-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models/\n",
    "\n",
    "https://theaisummer.com/transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
