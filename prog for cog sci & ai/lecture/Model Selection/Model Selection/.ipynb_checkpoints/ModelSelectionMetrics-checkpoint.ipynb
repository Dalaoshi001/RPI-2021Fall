{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for choosing the best model from a set of models (for Linear Regression)\n",
    "\n",
    "We want the model with the lowest test error but pure training error is a poor estimate of test error. \n",
    "\n",
    "To determine model with best generalization by **adjusting** training error we can penalizes the training error for more complex models (i.e. models with more predictors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        crim   zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "501  0.06263  0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273     21.0   \n",
       "502  0.04527  0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273     21.0   \n",
       "503  0.06076  0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273     21.0   \n",
       "504  0.10959  0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273     21.0   \n",
       "505  0.04741  0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273     21.0   \n",
       "\n",
       "      black  lstat  medv  \n",
       "501  391.99   9.67  22.4  \n",
       "502  396.90   9.08  20.6  \n",
       "503  396.90   5.64  23.9  \n",
       "504  393.45   6.48  22.0  \n",
       "505  396.90   7.88  11.9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = pd.read_csv('Boston.csv')\n",
    "boston.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13) (127, 13) (379,) (127,)\n"
     ]
    }
   ],
   "source": [
    "X = boston.iloc[:,0:-1].values\n",
    "y = boston.iloc[:,-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1234)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "model = LinearRegression().fit(X_train,y_train)\n",
    "yhat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 2) (127, 2) (379,) (127,)\n"
     ]
    }
   ],
   "source": [
    "X2 = X[:,0:2]\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size = 0.25, random_state = 1234)\n",
    "print(X_train2.shape, X_test2.shape, y_train2.shape, y_test2.shape)\n",
    "\n",
    "model2 = LinearRegression().fit(X_train2,y_train2)\n",
    "yhat2 = model2.predict(X_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted $R^2$: \n",
    "\n",
    "We can't use $R^2$ as a metric to compare models since it will always decrease as the number of predictors increases. We adjust $R^2$ for the number of predictors.\n",
    "    \n",
    "$$\\text{Adjusted }R^2 = 1 - \\frac{RSS/(n - d - 1)}{TSS/(n - 1)} = 1 - \\frac{(1 - R^2)(n-1)}{(n - d -1)}$$\n",
    "\n",
    "n: number of observations  \n",
    "d: number of predictors(include intercept)  \n",
    "RSS: Residual Sum of Squares  \n",
    "TSS: Total Sum of Squares  \n",
    "\n",
    "Adjusted R2 is always less than or equal to R2. \n",
    "Adjusted R2 = 1 indicates a model that perfectly predicts the target values and <= 0 indicates a model that has no predictive value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss(y,yhat):\n",
    "    return np.sum((y - yhat)**2)\n",
    "    \n",
    "def tss(y):\n",
    "    return(np.sum((y - np.mean(y))**2))\n",
    "\n",
    "def R_squared(y,yhat):\n",
    "    return(1 - (rss(y,yhat)/tss(y)))\n",
    "\n",
    "def adjr2_1(y,yhat,d):\n",
    "    n = len(y)\n",
    "    return 1 - ((rss(y,yhat)/(n-d-1))/(tss(y)/(n-1)))\n",
    "\n",
    "def adjr2_2(y,yhat,d):\n",
    "    n = len(y)\n",
    "    return 1 - (((1-R_squared(y,yhat))*(n-1))/(n-d-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7301842893511348"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_squared(y_train,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7205744147252848, 0.7205744147252848)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjr2_1(y_train,yhat,X.shape[1]),adjr2_2(y_train,yhat,X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7325732922440744"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_test = model.predict(X_test)\n",
    "R_squared(y_test,yhat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "#### Akaike Information Criteria (AIC)\n",
    "\n",
    "The value is only meaningful in comparison to other models. The lowest AIC is the best.\n",
    "\n",
    "$$AIC = -2logL+ 2d$$ \n",
    "\n",
    "L: Maximum Likelihood Estimate  \n",
    "d: number of predictors(include intercept)  \n",
    "\n",
    "For Linear Regression assuming the errors are Normally distributed\n",
    "\n",
    "$$AIC = nlog(RSS/n) + 2d$$\n",
    "\n",
    "n: number of observations  \n",
    "d: number of predictors(include intercept)  \n",
    "RSS: Residual Sum of Squares \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aic(y,yhat,d):\n",
    "    n = len(y)\n",
    "    return n*np.log(rss(y,yhat)/n) + 2*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1182.3188726789276"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aic(y_train,yhat,X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1557.4861130508261"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aic(y_train2,yhat2,X_train2.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Information Criteria (BIC)\n",
    "\n",
    "The value is only meaningful in comparison to other models. The lowest BIC is the best.\n",
    "\n",
    "In general, BIC imposes a heavier penalty than AIC for more predictors.\n",
    "\n",
    "$$BIC = -2log(L)+ dlog(n)$$\n",
    "\n",
    "L: Maximum Likelihood Estimate  \n",
    "n: number of observations  \n",
    "d: number of predictors(include intercept)  \n",
    "\n",
    "For Linear Regression assuming the errors are Normally distributed\n",
    "\n",
    "$$BIC = nlog(RSS/n) + dlog(n)$$\n",
    "\n",
    "n: number of observations  \n",
    "d: number of predictors(include intercept)  \n",
    "RSS: Residual Sum of Squares \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic(y,yhat,d):\n",
    "    n = len(y)\n",
    "    return n*np.log(rss(y,yhat)/n) + d*np.log(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1233.5068433449992"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bic(y_train,yhat,X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1565.361185460991"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bic(y_train2,yhat2,X2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1587.6427984724191, 1642.5877751731562)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LinearRegression().fit(X,y)\n",
    "y_hat = m.predict(X)\n",
    "a = aic(y,y_hat,X.shape[1])\n",
    "b = bic(y,y_hat,X.shape[1])\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See:\n",
    "\n",
    "https://math.stackexchange.com/questions/2093369/bayesian-information-criterion-derivation-for-linear-regression\n",
    "\n",
    "for a derivation of $nlog(RSS/n)$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
