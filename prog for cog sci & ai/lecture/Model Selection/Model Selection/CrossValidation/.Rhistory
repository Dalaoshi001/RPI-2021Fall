is.data.frame(airquality)
s = read.table("http://www.ats.ucl.edu/stat/examples/angell.txt")
df <- data.frame(a=c(1,2,3),b=c("a","b","c"),stringsAsFactors = F)
str(df)
df <- data.frame(a=c(1,2,3),b=c("a","b","c"))
str(df)
View(airquality)
d = c(3,7,9,10,16,20,25,57,30)
var(d)
View(anscombe)
d = [3,7,9,10,16,20,25,57,30]
d = c(3,7,9,10,16,20,25,57,30)
var(d)
var(1:10)
myvar=function(v){sum((v-mean(v))^2)/(length(v) -1)}
myvar(1:10)
sd(d)
sqrt(273.5)
View(tree.oj)
3+4
library("bayesplot", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library(bayesplot)
x < -5
x <-5
5
c(1,2,3)
x=5
x[1]
typeof(5)
mode(5)
typeof(c(1,2,3))
mode(c(1,2,3))
typeof(1L)
View(airquality)
airquality$Ozone
library(help='datasets')
head(airquality$Temp)
rm(list = ls())
devtools::install_github("gsimchoni/CastleOfR")
library(CastleOfR)
startGame()
openDoor(1)
(2^3)*(10 - 2)
?takeObject
?wtf
wtf()
takeObject(1)
?takeObject(2)
takeObject(2)
c(1,2,3)
k
wtf()
?k
install.packages("gapminder")
library(gapminder)
nrows(gapminder)
nrow(gapminder)
library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
installed.packages()
View(airquality)
mtcars
View(mtcars)
apply(cars,2,mean)
typeof(apply(cars,2,mean))
v = replicate(1000, mean(runif(5,0,100)))
df = iris[iris$Species == 'setosa' & iris$Sepal.Length > 5.0]
ifelse(airquality$Temp>50,1,0)
rm(list=ls())
X = airquality[,1:4]
ifelse(is.na(X$Ozone),
1,
X$Ozone)
ifelse(X$Ozone,mean(X$Ozone,na.rm=TRUE),X$Ozone)
ifelse(is.na(X$Ozone),
sapply(X$Ozone,function(z) print(z) mean(X$Ozone,na.rm=TRUE)),
X$Ozone)
ifelse(is.na(X$Ozone),
sapply(X$Ozone,function(z) {print(z); mean(X$Ozone,na.rm=TRUE)}),
X$Ozone)
ifelse(is.na(X$Ozone),
sapply(X$Ozone,function(z) {print(z); mean(X$Ozone,na.rm=TRUE)}),
function() {print(X$Ozone);X$Ozone})
X = airquality[,1:4]
View(X)
m = mean(X$Ozone,na.rm=TRUE)
for (i in length(X$Ozone)) {
if (is.na(X$Ozone[i])) X$Ozone[i] = m
}
head(X)
for (i in length(X$Ozone)) {
if (is.na(X$Ozone[i])) {print(X$Ozone[i]);X$Ozone[i] = m}
}
is.na(X$Ozone[5])
for (i in length(X$Ozone)) {
if (is.na(X$Ozone[i])) print(i)
}
for (i in length(X$Ozone)) {
if (is.na(X$Ozone[i])) print(i) else print(1)
}
apply(cars,2,mean)
install.packages("devtools")
49.5/60
52/60
g = c(52,
48,
58.5,
46,
62,
49.5,
56,
40,
48,
61.5,
61,
56,
46,
56,
59,
60,
37,
62,
36.75)
hist(g)
hist(g)
sapply(20:30,function(x) x/30.0)
sapply(18:30,function(x) x/30.0)
sapply(18:30,function(x) c(x,round(x/30.0,2)))
sapply(18:30,function(x) c(x,round(x/30.0,2))) -> G
table(G)
df = data.frame(G)
View(df)
df = data.frame(t(G))
X = matrix(c(1,2,3,1,4,9),ncol=2)
X
.5+4+4+2+.5+1+.75+6+4+8
64-30.75
53/60
58/60
57/60
47/51
47/60
52/60
51/60
x = matrix(c(1,2,3,4,5),nrow=2)
x = matrix(c(1,2,3,4,5),nrow=2,ncol=3)
x = matrix(c(1,2,3,4,5),nrow=2,ncol=5)
dim(x)
diag(x)
x
trace(x)
sum(diag(x))
View(mtcars)
str(mtcars)
m = lm(mpg~disp+hp+drat+wt+qsec,data=tcars)
m = lm(mpg~disp+hp+drat+wt+qsec,data=mtcars)
m
summary(m)
m2 = lm(mpg~disp,data=mtcars)
summary(m2)
cor(mtcars$wt,mtcars$disp)
cor(mtcars$wt,mtcars$hp)
cor(mtcars$wt,mtcars$drat)
m = lm(mpg~disp+hp+wt+qsec,data=mtcars)
summary(m)
fn = file.choose()
fn
df = read.csv(fn)
View(df)
m = lm(pulse~exercise,data=df)
summary(m)
plot(m)
setwd("~/Courses/Programming-2017/Materials/Data")
# Importing the dataset
df = read.csv('PurchaseData.csv')
head(df)
# Encode the dependent variable as a factor
df$Purchased = factor(df$Purchased, levels = c(0, 1),labels = c("no","yes"))
str(df)
# Split the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(1234)
split = sample.split(df$Purchased, SplitRatio = 0.75)
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
foo = train
f[2,3] = scale(foo[2,3])
foo[2,3] = scale(foo[2,3])
View(foo)
foo = train
foo[,2:3] = scale(foo[,2:3])
View(foo)
rm(foo)
# Feature Scaling
train[,2:3] = scale(training_set[,2:3])
test[,2:3] = scale(test_set[,2:3])
head(train[2:3])
foo = df
colnames(df$ExtimatedSalary) = "Salary"
colnames(df)[3] = "Salary"
head(foo)
head(df)
write.csv(df,"PurchaseData.csv",row.names=FALSE)
# Importing the dataset
df = read.csv('PurchaseData.csv')
# Encode the dependent variable as a factor
df$Purchased = factor(df$Purchased, levels = c(0, 1),labels = c("no","yes"))
library(caTools)
library(e1071)
set.seed(1234)
split = sample.split(df$Purchased, SplitRatio = 0.75)
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
# Feature Scaling
train[,2:3] = scale(training_set[,2:3])
test[,2:3] = scale(test_set[,2:3])
# Feature Scaling
train[,2:3] = scale(train[,2:3])
test[,2:3] = scale(test[,2:3])
View(test)
# Importing the dataset
df = read.csv('PurchaseData.csv')
View(df)
foo = read.csv("Social_Network_Ads.csv")
foo = foo[2:5]
View(foo)
colnames(foo)[3] = "salary"
colnames(foo)[3] = "Salary"
write.csv(foo,"PurchaseData.csv",row.names=FALSE)
rm(list=ls())
# Importing the dataset
df = read.csv('PurchaseData.csv')
# Encode the dependent variable as a factor
df$Purchased = factor(df$Purchased, levels = c(0, 1),labels = c("no","yes"))
View(df)
library(caTools)
library(e1071)
set.seed(1234)
split = sample.split(df$Purchased, SplitRatio = 0.75)
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
# Feature Scaling
train[,2:3] = scale(train[,2:3])
test[,2:3] = scale(test[,2:3])
View(test)
# Fit to the Training data
m = naiveBayes(X = train[], y = training_set$Purchased)
# Fit to the training data
m = naiveBayes(X = train[2:3], y = train$Purchased)
# Fit to the training data
m = naiveBayes(x = train[2:3], y = train$Purchased)
# Predict the test data
preds= predict(m, newdata = test[2:3])
m
# Make Confusion Matrix and calculate accuracy
cm = table(test_set[, 3], y_pred)
# Make Confusion Matrix and calculate accuracy
cm = table(test$Purchased, preds)
cm
accuracy = sum(diag(cm))/sum(cm)
accuracy
head(iris)
library(caTools)
library(class)
df = iris
split = sample.split(df$Species, SplitRatio = 0.8)
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
preds1 = knn(train = train[, -5],test = test[, -5],cl = train[, 5],k = 1,prob = TRUE)
preds1
View(train)
View(train)
View(train)
View(test)
View(train)
foo = subset(train,train$Species == setosa)
foo = subset(train,train$Species == "setosa")
View(foo)
library(caTools)
library(class)
df = iris
split = sample.split(df$Species, SplitRatio = 0.5)
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
preds1 = knn(train = train[, -5],test = test[, -5],cl = train[, 5],k = 1,prob = TRUE)
preds5 = knn(train = train[, -5],test = test[, -5],cl = train[, 5],k = 5,prob = TRUE)
preds10 = knn(train = train[, -5],test = test[, -5],cl = train[, 5],k = 10,prob = TRUE)
cm1 = table(test[, 5], preds1)
test_error1 = 1 - (sum(diag(cm1))/sum(cm1))
cm5 = table(test[, 5], preds5)
test_error5 = 1 - (sum(diag(cm5))/sum(cm5))
cm10 = table(test[, 5], preds10)
test_error10 = 1 - (sum(diag(cm10))/sum(cm10))
cm1
cm5
cm10
c(test_error1,test_error5,test_error10)
library(e1071)
getwd()
library(MASS)
plot(Boston$age,Boston$medv)
plot(Boston$rm,Boston$medv)
View(mpg)
library(ggplot2)
View(mpg)
cm
# Support Vector Machine (SVM)
library(caTools)
library(e1071)
set.seed(1234)
df = read.csv('PurchaseData.csv')
# Encode the dependent variable as a factor
df$Purchased = factor(df$Purchased, levels = c(0, 1),labels = c("no","yes"))
# Split the dataset into the Training set and Test set
split = sample.split(df$Purchased, SplitRatio = 0.75)
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
# Scale the features
train[,2:3] = scale(train[,2:3])
test[,2:3] = scale(test[,2:3])
m = svm(Purchased ~ Age + Salary,data = train,type = 'C-classification',kernel = 'linear')
# Predict the Test data
preds = predict(m, newdata = test[2:3])
# Make the Confusion Matrix and calculate accuracy
cm = table(test$Purchased, preds)
cm
cmm = table(preds,test$Purchased)
cmm
preds
tables(preds)
table(preds)
cm
cmm
contrasts(m)
table(test$Purchased)
table(test$Purchased)[0]
table(test$Purchased)[1]
cm[1]
cm
cm[2]
cm[3]
cm[4]
table(preds)
#(TP+TN)/(TP+TN+FP+FN)
accuracy = function(cm) {sum(diag(cm))/sum(cm)}
accuracy(cm)
#TP/(TP+FP)
precision = function(cm, preds.by.col = TRUE) {cm[4]/(cm[4]+cm[2])}
precision(cm)
cm
cm[2]
cmm
#TP/(TP+FN)
recall = function(cm, preds.by.col = TRUE) {cm[4]/(cm[4]+cm[2])}
recall(cm)
F1 = function(cm){2*precision(c)*recall(cm)/(precision(cm)+recall(cm))}
#TP/(TP+FP)
precision = function(cm, preds.by.col = TRUE) {cm[4]/(cm[4]+cm[3])}
precision(cm)
F1(cm)
F1 = function(cm,preds.by.col = TRUE){2*precision(cm)*recall(cm)/(precision(cm)+recall(cm))}
F1(cm)
#TP/(TP+FP)
precision = function(cm, preds.by.col = TRUE) {
cm[4] / if (preds.by.col) (cm[4]+cm[3]) else  (cm[4]+cm[2])}
cmm
precision(cm)
#TP/(TP+FN)
recall = function(cm, preds.by.col = TRUE) {
cm[4] / if (preds.by.col) (cm[4]+cm[2]) else  (cm[4]+cm[3])}
F1 = function(cm,preds.by.col = TRUE){2*precision(cm)*recall(cm)/(precision(cm)+recall(cm))}
F1(cm)
cm
source('~/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/SVR/svr.R')
setwd("~/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/SVR")
source('~/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/SVR/svr.R')
rm(list=ls())
source('~/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/SVR/svr.R')
# Visualising the SVR results
# install.packages('ggplot2')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.1)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
summary(regressor)
x <- seq(0.1, 5, by = 0.05)
y <- log(x) + rnorm(x, sd = 0.2)
# estimate model and predict input values
m   <- svm(x, y)
new <- predict(m, x)
# visualize
plot(x, y)
points(x, log(x), col = 2)
points(x, new, col = 4)
plot(x, y)
points(x, log(x), col = 2)
points(x, new, col = 4)
summary(m)
View(dataset)
View(dataset)
getwd()
setwd("~/Courses/Programming-2017/Lectures/Classification/SVM")
getwd()
write.csv(dataset,"Salaries.csv",row.names=FALSE)
rm(list=ls())
# Kernel SVM
library(caTools)
library(e1071)
set.seed(1234)
df = read.csv('PurchaseData.csv')
# Encoding the target feature as factor
df$Purchased = factor(df$Purchased, levels = c(0, 1),labels = c("no","yes"))
# Split the data
split = sample.split(df$Purchased, SplitRatio = 0.75)
training = subset(df, split == TRUE)
test = subset(df, split == FALSE)
# Scale the features
train[,2:3] = scale(train[,2:3])
test[,2:3] = scale(test[,2:3])
# Fit Model
m = svm(formula = Purchased ~ .,data = train,
type = 'C-classification', kernel = 'radial')
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
# Scale the features
train[,2:3] = scale(train[,2:3])
test[,2:3] = scale(test[,2:3])
# Fit Model
m = svm(formula = Purchased ~ .,data = train,
type = 'C-classification', kernel = 'radial')
# Predict the test data
preds = predict(m, newdata = test_[2:3])
# Predict the test data
preds = predict(m, newdata = test[2:3])
View(test)
View(df)
# Fit Model
m = svm(formula = Purchased ~ Age + Salary,data = train,
type = 'C-classification', kernel = 'radial')
# Predict the test data
preds = predict(m, newdata = test[2:3])
# Making the Confusion Matrix
cm = table(test$Purchased, preds)
cm
accuracy = function(cm) {sum(diag(cm))/sum(cm)} #(TP+TN)/(TP+TN+FP+FN)
accuracy(cm)
# Visualise the Training
library(ElemStatLearn)
set = training
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
# Visualise the Training
library(ElemStatLearn)
set = train
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
set = subset(df, split == TRUE)
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
set = train
X1 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
X2 = seq(min(set[, 3]) - 1, max(set[, 3]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(m, newdata = grid_set)
plot(set[, 2:3],
main = 'Kernel SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
View(grid_set)
View(df)
View(df)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'Salary')
y_grid = predict(m, newdata = grid_set)
plot(set[, 2:3],
main = 'Kernel SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 4] == 1, 'green4', 'red3'))
View(set)
View(set)
View(set)
plot(set[, 2:3],
main = 'Kernel SVM (Training set)',
xlab = 'Age', ylab = 'Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 4] == "yes", 'green4', 'red3'))
getwd()
setwd("~/Downloads")
df = read.csv("auto_mpg.csv")
df = read.csv("auto_mpg.csv")
View(df)
View(df)
setwd("~/Courses/Programming-2017/Lectures/Resampling Methods")
