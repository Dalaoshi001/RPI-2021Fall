{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "\n",
    "Convolutional Neural Networks were developed by Yann LeCun (1989,1998). The first CNN was LeNet-5 which recognized hand-written digits and words. It acjieved 99.2% accuracy on MINST dataset.\n",
    "\n",
    "https://en.wikipedia.org/wiki/LeNet\n",
    "\n",
    "AlexNet was developed by Alex Krizhevsky,Ilya Sutskever, and Geoffrey Hinton (2012). AlexNet won the ImageNet Challenge which was a major milestone in AI and got the attention of Vision researchers who previously were very critical of Machine Learning.\n",
    "\n",
    "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "\n",
    "![](Hinton1.png)\n",
    "\n",
    "The ImageNet Challenge consists of 1000 Categories with 1.2M Training examples and 100,000 Test examples. A model output the top five class probabilities\n",
    "\n",
    "AlexNet won 1st Place with a top five error rate of 15.3%. The second best entry had an error rate of 26.2%.\n",
    "\n",
    "https://www.image-net.org/update-mar-11-2021.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Architecture\n",
    "\n",
    "![](CNN_Architecture.jpg)\n",
    "![](FullCNN1.png)\n",
    "\n",
    "Source: towardsdatascience.com\n",
    "\n",
    "### Feature Learning (Representation Learning)\n",
    "\n",
    "Representation (or Feature) learning learns the best way to represent the data to detect features.\n",
    "\n",
    "Feature learning automatically learns which features are important rather than the features being manually engineered.\n",
    "\n",
    "This was a  major achievement since manual feature engineering is time consuming and prone to human biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "#### Why Convolution?\n",
    "\n",
    "Convolution is location invariant. This is important in order to identify an object by its local context (i.e. locality)\n",
    "\n",
    "The convolution function combines two functions in time. The shape of one function is modified by the other\n",
    "\n",
    "$$(f*g)(t) = \\int_{-\\infty}^{\\infty}f(\\tau)(t - \\tau)d\\tau$$\n",
    "\n",
    "http://mathworld.wolfram.com/Convolution.html\n",
    "\n",
    "\n",
    "\n",
    "Technically the convolution operation in a CNN is Cross-Correlation. Cross-correlation slides a kernel over the image; convolution slides a flipped kernel over the image.\n",
    "\n",
    "![](CrossCorr.png)\n",
    "\n",
    "$\\text{Comparsion of Cross Correlation and Convolution. Source: Wikipedia}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Image\n",
    "\n",
    "An input image is typically:\n",
    "\n",
    "- 256x256 for grey scale images\n",
    "- 256x256x3 for color (RGB) images\n",
    "\n",
    "![](Convolution1.png)\n",
    "\n",
    "Stride is how much to move the feature detector sideways and down. 2 is a popular choice. Stride reduces dimensionality.\n",
    "    \n",
    "Padding puts a border of zeros completely around the input image. Usually a single or double border.\n",
    "\n",
    "https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "\n",
    "#### Feature Detector (Kernel, Filter)\n",
    "\n",
    "A feature detector convolves the kernel with the image to detects different features in the image such as edges or lines.\n",
    " \n",
    "Feature detectors are organized in layers (e.g. 32 layers, 1 feature detector per layer).\n",
    "    \n",
    "**CNNs learn the best feature detectors in the same way the weights are learned**\n",
    "\n",
    "http://setosa.io/ev/image-kernels/\n",
    "    \n",
    "#### Feature Map\n",
    "\n",
    "A feature map is a reduced form of image. Some information is lost but the important information (e.g. the features) is preserved.\n",
    "\n",
    "![](Convolution2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Correlation Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    print(X.shape,K.shape,Y.shape)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "torch.Size([3, 3]) torch.Size([2, 2]) torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = torch.Tensor([[0, 1], [2, 3]])\n",
    "print(X)\n",
    "print(K)\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Detection\n",
    "\n",
    "1. to 0. is a white to black edge, 0.to 1. is a black to white edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.Tensor([[1, -1]])  ## Detects vertical edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 8]) torch.Size([1, 2]) torch.Size([6, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn edge detection kernel K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2, loss 5.215\n",
      "batch 4, loss 1.196\n",
      "batch 6, loss 0.332\n",
      "batch 8, loss 0.110\n",
      "batch 10, loss 0.040\n"
     ]
    }
   ],
   "source": [
    "# 1 input channel, 1 output channel, kernel shape of (1, 2)\n",
    "\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2),bias=False) #Ignoring bias\n",
    "\n",
    "# The 2-d convolutional layer uses four-dimensional input and output\n",
    "# number of examples , number of channels, height, width)\n",
    "\n",
    "X_ = X.reshape((1, 1, 6, 8))\n",
    "Y_ = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "alpha = 0.03  # Learning Rate\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    conv2d.zero_grad() # Zero gradients\n",
    "    Y_hat = conv2d(X_) # Forward pass\n",
    "    loss = ((Y_hat - Y_) ** 2).sum()  # Calc Loss\n",
    "    loss.backward() # Calc Gradients\n",
    "    conv2d.weight.data[:] -= alpha * conv2d.weight.grad # Gradient Descient\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print('batch %d, loss %.3f' % (epoch + 1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0097, -0.9698]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(conv2d.weight.data.shape)\n",
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU activation function\n",
    "\n",
    "A ReLU activation function is applied to increase non-linearity in the network since Convolution is linear operation.\n",
    "\n",
    "The ReLU is applied element-wise to the feature map to remove negative values, e.g. remove all the black pixels to sharpen the border between objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling (Downsampling)\n",
    "\n",
    "Downsampling provides spatial invariance.\n",
    "\n",
    "The types of Pooling are:\n",
    "\n",
    "- Max\n",
    "- Min\n",
    "- Sum\n",
    "- Mean\n",
    "- Subsampling\n",
    "\n",
    "#### Max Pooling \n",
    "\n",
    "Max pooling is the most popular. Below is a 2x2  filter with stride = 2.\n",
    "\n",
    "![](MaxPooling1.png)\n",
    "\n",
    "\n",
    "![](MaxPooling2.png)\n",
    "\n",
    "Pooling removes information but preserves features. It accounts for distortions and reduces the size and therefore reduces the number of parameters. It helps to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float32)\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch MaxPool2d \n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "\n",
    "\n",
    "#### Pooling with Stride\n",
    "\n",
    "By default the stride has the same shape as the pooling window shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]]]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3) # Pooling and stride shape is 3x3, no padding\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning Issue. https://github.com/pytorch/pytorch/issues/60053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  3.],\n",
       "          [ 9., 11.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3)) # pad should be smaller than half of kernel size\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 4])\n",
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]],\n",
      "\n",
      "         [[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.cat((X, X + 1), dim=1)\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Convolution and Pooling\n",
    "\n",
    "http://scs.ryerson.ca/~aharley/vis/conv/flat.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening\n",
    "\n",
    "Flattening converts the final pooled feature maps to a 1-d array by row.\n",
    "\n",
    "![](flattening.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected ANN\n",
    "\n",
    "The next step is a fully connected network to combine features.\n",
    "\n",
    "![](FullAnn2.png)\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "In the backpropagation step the **Feature detectors are adjusted** as well as the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout is a regularization technique. At each training epoch, individual units with their incoming and outgoing edges are randomly dropped from the network structure.\n",
    "\n",
    "This lessens overfitting by reducing the interdependency of the units.\n",
    "\n",
    "![](Dropout.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
