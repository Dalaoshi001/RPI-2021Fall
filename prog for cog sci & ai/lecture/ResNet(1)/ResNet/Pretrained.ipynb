{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400052c1-3e26-4469-8b8b-21d6e0235f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fd7c1b-7324-4973-9802-e1b51972b2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'DenseNet',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'MNASNet',\n",
       " 'MobileNetV2',\n",
       " 'MobileNetV3',\n",
       " 'ResNet',\n",
       " 'ShuffleNetV2',\n",
       " 'SqueezeNet',\n",
       " 'VGG',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenetv2',\n",
       " 'mobilenetv3',\n",
       " 'quantization',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'utils',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbbd3c9-6971-4db4-9905-6c2bd0b8a10d",
   "metadata": {},
   "source": [
    "#### Load the file containing the 1,000 labels for the ImageNet dataset classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26349ec3-e53f-4551-81ee-8209f1b85c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9763e1-20e1-4306-b6ec-eaff0142e6bf",
   "metadata": {},
   "source": [
    "#### Setup image transform\n",
    "\n",
    "https://pytorch.org/vision/stable/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "274be7a3-03ea-4736-bbb4-036806a2a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Reshape, crop, and normalize \n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # 3 color channels\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80ff2a-785e-40b1-afca-527811340dcb",
   "metadata": {},
   "source": [
    "#### Download the pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea1259f5-64c6-47f7-b477-abe2624a027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res101 =  models.resnet101(pretrained=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec68c12c-24b4-4081-b5a6-293a75f5c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alex = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f80b36-a0e9-4660-a399-8ae8e0103a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "res50 = models.resnet50(pretrained=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "318ce71b-10f8-4143-a2c5-80ad59b28583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg19 = models.vgg19(pretrained=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f70c3e-df6a-4903-9b3d-34e61599550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al /Users/mike/.cache/torch/hub/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d806bc-6f8d-4e6d-8613-452317d7c6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = res101\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b7acb-7bdd-47c7-9062-b06031b30445",
   "metadata": {},
   "source": [
    "#### Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c3180-89aa-491e-869d-eeb4a112cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095c96e-ce63-482f-a303-54ac01a00f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"images/dog.png\").convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a19707-66ec-4c48-8911-0daeeb352d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsiz = img.size\n",
    "print(img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49c9f1-fd32-4180-ada8-b6e93f320160",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "GGcs-W5Tgm1r",
    "outputId": "98fd7228-7c68-4c6c-b5ba-9c965b4fa74e"
   },
   "outputs": [],
   "source": [
    "print(summary(model,torch.zeros((1,3,imgsiz[0],imgsiz[1])),show_input=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa0c9c-fc9b-42e9-b2ed-274cbc62c006",
   "metadata": {},
   "source": [
    "#### Preprocess the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2947ba-5d3d-47e8-a758-1a6c2b9dba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_prep = preprocess(img)\n",
    "print(img_prep.shape)\n",
    "batch_img = torch.unsqueeze(img_prep, 0)\n",
    "batch_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb167c7-9a74-4772-a86b-ad52becc4420",
   "metadata": {},
   "source": [
    "#### Predict image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711cd66-5759-4c07-b892-20df6e015a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Set evaluation mode\n",
    "preds = model(batch_img)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8314b-9cf1-453f-afe1-1f9b7326c24f",
   "metadata": {},
   "source": [
    "#### Map predictions to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2b6cc-f404-4aa2-bb31-51c6cdc9a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = torch.max(preds, 1)\n",
    "\n",
    "probs = torch.nn.functional.softmax(preds, dim=1)[0] * 100\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64f09e-95e3-45fc-b910-b2a02a8f7551",
   "metadata": {},
   "source": [
    "#### Print label and probability of top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8149e-b285-4bd3-a223-01129854df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[index[0]], probs[index[0]].item()) # Index 0 is top choice\n",
    "\n",
    "_, indices = torch.sort(preds, descending=True)\n",
    "[(labels[idx], probs[idx].item()) for idx in indices[0][:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf560f4-5ed4-4686-b7a7-82a06007d8c7",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "\n",
    "https://vitalflux.com/pytorch-load-predict-pretrained-resnet-model/\n",
    "https://learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a2ee3-0937-499c-8105-531ce0765eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
