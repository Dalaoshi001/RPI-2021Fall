{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "\n",
    "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "\n",
    "#### Data Augmentation\n",
    "\n",
    "Since AlexNet has 60M Parameters trained on 1.2M samples we should suspect that overfitting is a problem. One way to combat overfitting is to artificially enlarge the training set by data augmentation.\n",
    "\n",
    "Alexnet has two forms of data augmentation: 1. Image translation and horizontal reflection. 2. Alter intensity of the RGB channels.\n",
    "\n",
    "![](alexnet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same Convolution\n",
    "\n",
    "A Same Convolution is a type of convolution where the output matrix is of the same dimension as the input matrix.\n",
    "\n",
    "For a nxn input matrix A and a fxf filter matrix F: the output of the convolution A*F is of dimension: \n",
    "$$\\left(\\frac{n*2p-f}{s}\\right)+1 \\text{ x } \\left(\\frac{n*2p-f}{s}\\right)+1$$\n",
    "s = stride   \n",
    "p = padding\n",
    "\n",
    "For a same convolution:\n",
    "- s = 1,  \n",
    "- p = $\\frac{f - 1}{2}$, and   \n",
    "- f is an odd number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16\n",
    "\n",
    "Karen Simonyan and Andrew Zisserman (2014). Visual Geometry Group Lab of Oxford University.\n",
    "\n",
    "https://arxiv.org/abs/1409.1556\n",
    "\n",
    "Goal of the research was to analyze how to increase the depth of Convolutional Networks.\n",
    "\n",
    "All convolutions are same convolutions.\n",
    "\n",
    "![](VGG.png)\n",
    "\n",
    "\n",
    "\n",
    "* ~138 Million parameters\n",
    "* 3x3 filters\n",
    "* Stride = 1\n",
    "* Number of filters 64->128->256->512\n",
    "\n",
    "Notice that the number of channels increases (increasing the number of parameters) and the size decreases (to reduce the number of parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "He,Zhang,Ren and Sun (2015) [Deep Residual Learning for Image Recognition. ](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "Why doesn't adding more layers improve Training and Test Error? \n",
    "\n",
    "Weight decay, small random initialization, L2 regularization biased the learning toward zero. As you add layers the model tends to learn the zero function, F(x) = 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](MoreLayers.png)\n",
    "\n",
    "$\\text{Train and Test Error for 20 and 56 layer networks}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Blocks\n",
    "\n",
    "![](ResNetBlk.png)\n",
    "\n",
    "$\\text{Residual Building Block}$\n",
    "\n",
    "\n",
    "\n",
    "Solution: Make the identity function rather than the zero function as the default function.\n",
    "\n",
    "Learn F(x) + x, F(x) is the change to x made by the layer(s).\n",
    "\n",
    "![](ResBlock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 115%;\">\n",
    "$$a^{l+2} = ReLU((W^{l+2}\\cdot{a^{l+1}}+b^{l+2}) + a^l)$$\n",
    "</div>\n",
    "\n",
    "If the weights and bias = 0 (because of weight decay, small random initialization, L2 regularization) then\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ a^l = ReLU(a^l) = a^l$$\n",
    "</div>\n",
    "\n",
    "The Residual block learns the indentity function. It is called a \"skip\" or \"short cut\" connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet Architecture\n",
    "\n",
    "![](ResNet.png)\n",
    "\n",
    "VGG-19 has 19.6 Billion FLOPs whereas the 34 layer ResNet has 3.6 Billion FLOPs. Thus it is less complex eventhough it is deeper. Each layer only learns a little so don't need as many parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1x1 Convolutions\n",
    "\n",
    "https://medium.com/analytics-vidhya/talented-mr-1x1-comprehensive-look-at-1x1-convolution-in-deep-learning-f6b355825578\n",
    "\n",
    "Dotted line in the ResNet architecture are where the number of channels increases.\n",
    "\n",
    "A 1x1 convolution projects to a higher number of channels but doesn't change the input size.\n",
    "\n",
    "![](Bottleneck.png)\n",
    "\n",
    "$\\text{Bottleneck Building Block}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Ioffe and Szegedy (2015) [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "In each training iteration, BN normalizes the output of each hidden layer node \n",
    "(on each layer where it is applied) by subtracting its mean and dividing by its standard deviation, estimating both based on the current minibatch.\n",
    "\n",
    "For each Hidden Layer on which Batch Normalization is applie:\n",
    "\n",
    "![](BatchNorm.png)\n",
    "\n",
    "For convolutional layers, batch normalization occurs after the convolution computation and before the application of the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, DiveIntoDeepLearning\n",
    "\n",
    "Andrew Ng, DeepLearning.AI\n",
    "\n",
    "Jason Brownlee, A Gentle Introduction to 1Ã—1 Convolutions to Manage Model Complexity\n",
    "\n",
    "https://www.youtube.com/watch?v=GWt6Fu05voI&ab_channel=YannicKilcher"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
