{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    " \n",
    "* Response variable is Qualitative(i.e. Categorical)\n",
    "* Predict qualitative response\n",
    "* Some methods first predict probability that observation belongs to a category\n",
    "* Set of observations\n",
    "\n",
    "$(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$\n",
    "\n",
    "- Train on training data and predict on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default data set\n",
    "\n",
    "* Predict if a person will default on his/her credit card based on income and credit card balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>default</th><th scope=col>student</th><th scope=col>balance</th><th scope=col>income</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 729.5265</td><td>44361.625</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 817.1804</td><td>12106.135</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td>1073.5492</td><td>31767.139</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 529.2506</td><td>35704.494</td></tr>\n",
       "\t<tr><td>No       </td><td>No       </td><td> 785.6559</td><td>38463.496</td></tr>\n",
       "\t<tr><td>No       </td><td>Yes      </td><td> 919.5885</td><td> 7491.559</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " default & student & balance & income\\\\\n",
       "\\hline\n",
       "\t No        & No        &  729.5265 & 44361.625\\\\\n",
       "\t No        & Yes       &  817.1804 & 12106.135\\\\\n",
       "\t No        & No        & 1073.5492 & 31767.139\\\\\n",
       "\t No        & No        &  529.2506 & 35704.494\\\\\n",
       "\t No        & No        &  785.6559 & 38463.496\\\\\n",
       "\t No        & Yes       &  919.5885 &  7491.559\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "default | student | balance | income | \n",
       "|---|---|---|---|---|---|\n",
       "| No        | No        |  729.5265 | 44361.625 | \n",
       "| No        | Yes       |  817.1804 | 12106.135 | \n",
       "| No        | No        | 1073.5492 | 31767.139 | \n",
       "| No        | No        |  529.2506 | 35704.494 | \n",
       "| No        | No        |  785.6559 | 38463.496 | \n",
       "| No        | Yes       |  919.5885 |  7491.559 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  default student balance   income   \n",
       "1 No      No       729.5265 44361.625\n",
       "2 No      Yes      817.1804 12106.135\n",
       "3 No      No      1073.5492 31767.139\n",
       "4 No      No       529.2506 35704.494\n",
       "5 No      No       785.6559 38463.496\n",
       "6 No      Yes      919.5885  7491.559"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t10000 obs. of  4 variables:\n",
      " $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n",
      " $ balance: num  730 817 1074 529 786 ...\n",
      " $ income : num  44362 12106 31767 35704 38463 ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "library(ISLR)\n",
    "head(Default)\n",
    "str(Default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "    \n",
    "![](Classification.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Not Linear Regression?\n",
    "\n",
    "* Code response as 1 = default, 0 = no default\n",
    "* Line goes through 0 and 1, not a probability\n",
    "\n",
    "\n",
    "![](LogisticCurve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "* Models the probability that the response Y belongs to a particular category\n",
    "* Sigmoid shape function, asymtotes at 0 and 1 \n",
    "    - logistic function\n",
    "  \n",
    "\n",
    "$$p(X) = Pr(Y = 1|X) = \\frac{e^{\\beta_0 + \\beta_1X}} {1+e^{\\beta_0 + \\beta_1X}}$$\n",
    "\n",
    " \n",
    "#### Logit ( Log odds)\n",
    " \n",
    "* Odds: Probability of winning divided by probability of not winning, $\\frac{P(Win)}{P(Not Win)}$\n",
    "    - if probability of winning is .8,, .8/(1 - .8) = 4\n",
    "    - 4 to 1 odds, if odd are 4 t0 1, you will win 4 times out of 5\n",
    "    \n",
    "* Logit is the inverse of the logistic function: $logit(p) =  log(\\frac{p}{1-p})$\n",
    "    \n",
    "* Substitute for P(X) from above:\n",
    " \n",
    "$$\\frac{p(X)}{1 - p(X)} = \\frac{\\frac{e^{\\beta_0 + \\beta_1}} {1+e^{\\beta_0 + \\beta_1}}} {1 - \\frac{e^{\\beta_0 + \\beta_1}} {1+e^{\\beta_0 + \\beta_1}}}\n",
    "= \\frac{\\frac{e^{\\beta_0 + \\beta_1X}} {1+e^{\\beta_0 + \\beta_1X}}} {\\frac{1+e^{\\beta_0 + \\beta_1X}}{1+e^{\\beta_0 + \\beta_1X}} - \\frac{e^{\\beta_0 + \\beta_1X}} {1+e^{\\beta_0 + \\beta_1X}}}={e^{\\beta_0 + \\beta_1X}}$$\n",
    "  \n",
    "* Log odds: Take log of both sides:\n",
    " \n",
    "$$log(\\frac{p(X)}{1 - p(X)}) = \\beta_0 + \\beta_1 X$$\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Model (GLM)\n",
    "\n",
    "* Response being 0 or 1, binomial distribution, not normal\n",
    "* Logit is a transformation of response variable\n",
    "* To generalize linear models:\n",
    "    - Allow response distribution to be a member of the exponential family\n",
    "    - Allow transformations of response variable (link function)\n",
    "* glm function\n",
    "    - Specify a family: \n",
    "    - Logistic regression: family = \"binomial\", link = logit\n",
    "    - Poisson regression: family = \"poisson\", link = log\n",
    "    - Linear regression: family = \"normal\", link = identity\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the coefficients\n",
    "\n",
    "- Maximum Likelihood \n",
    "    - We want to estimate the coefficients such that when plugged into p(x), we get a number close to 1 for those individuals who defaulted and close to 0 for those who didn't\n",
    "- Maximize the likelihood function L($\\beta_0$,$\\beta_1$)\n",
    "    \n",
    "$$L(\\beta_0,\\beta_1) = \\Pi_{i:y_i=1}p(x_i)\\Pi_{j:y_j=0}(1 - p(x_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>-10.6513306138618</dd>\n",
       "\t<dt>balance</dt>\n",
       "\t\t<dd>0.00549891693091148</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] -10.6513306138618\n",
       "\\item[balance] 0.00549891693091148\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   -10.6513306138618balance\n",
       ":   0.00549891693091148\n",
       "\n"
      ],
      "text/plain": [
       "  (Intercept)       balance \n",
       "-10.651330614   0.005498917 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = default ~ balance, family = binomial, data = Default)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.2697  -0.1465  -0.0589  -0.0221   3.7589  \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -1.065e+01  3.612e-01  -29.49   <2e-16 ***\n",
       "balance      5.499e-03  2.204e-04   24.95   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 2920.6  on 9999  degrees of freedom\n",
       "Residual deviance: 1596.5  on 9998  degrees of freedom\n",
       "AIC: 1600.5\n",
       "\n",
       "Number of Fisher Scoring iterations: 8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "glm.fit1=glm(default~balance,data=Default,family=binomial)\n",
    "coef(glm.fit1)\n",
    "summary(glm.fit1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients\n",
    " \n",
    "* One unit increase in balance results in an increase in the log odds of default by 0.0055 units\n",
    "* Std. Error measures the accuracy of the coefficient estimates\n",
    "* z statistic plays same role as t statistic in regression $z = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})}$\n",
    "\n",
    "* Large z value is strong evidence against the null hypothesis that $\\beta_1$ = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### Deviance\n",
    "\n",
    "* A measure of goodness (badness) of fit in a GLM\n",
    "* Higher numbers imply worse fit\n",
    "* Null deviance: model with just the intercept\n",
    "* Residual deviance: full model\n",
    "    * Much better fit including balance\n",
    "  \n",
    "#### Akaike Information Criteria (AIC)\n",
    "\n",
    "* Assesses the quality of the model relative to other models\n",
    "* Select model with lowest AIC\n",
    " \n",
    "#### Fisher Scoring\n",
    "\n",
    "* Method for solving maximum liklihood problems\n",
    "* Converged in 8 iterations\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "$\\hat{P}(X) = \\frac{e^{-10.65 + 0.0055X}} {1+e^{-10.65 + 0.0055X}}$\n",
    "\n",
    "* If X = 1000, $\\hat{P}$(X) = 0.00576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>0.00575214508582021</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>0.585769369615355</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.00575214508582021\n",
       "\\item[2] 0.585769369615355\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.005752145085820212\n",
       ":   0.585769369615355\n",
       "\n"
      ],
      "text/plain": [
       "          1           2 \n",
       "0.005752145 0.585769370 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(glm.fit1,newdata=data.frame(balance=c(1000.00,2000.00)),type=\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-9.46506555]), array([[ 0.00478248]])]\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#%%\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Default.csv')\n",
    "X = dataset.iloc[:, 2].values\n",
    "y = dataset.iloc[:, 0].values\n",
    "X = X.reshape(-1,1)  # Classifier want a 2d array\n",
    "\n",
    "# Fitting Logistic Regression on default~balance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 1234)\n",
    "classifier.fit(X, y)\n",
    "print([classifier.intercept_,classifier.coef_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  ['No' 'Yes']\n",
      "Prediction Probabilities:  [[ 0.99082984  0.00917016]\n",
      " [ 0.4750484   0.5249516 ]]\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "X_pred = np.array((1000.0,2000.0)).reshape(2,1)\n",
    "y_pred = classifier.predict(X_pred)\n",
    "print(\"Predictions: \",y_pred)\n",
    "y_prob = classifier.predict_proba(X_pred)\n",
    "print(\"Prediction Probabilities: \",y_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using qualitative predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = default ~ student, family = binomial, data = Default)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-0.2970  -0.2970  -0.2434  -0.2434   2.6585  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -3.50413    0.07071  -49.55  < 2e-16 ***\n",
       "studentYes   0.40489    0.11502    3.52 0.000431 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 2920.6  on 9999  degrees of freedom\n",
       "Residual deviance: 2908.7  on 9998  degrees of freedom\n",
       "AIC: 2912.7\n",
       "\n",
       "Number of Fisher Scoring iterations: 6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "glm.fit = glm.fit=glm(default~student,data=Default,family=binomial)\n",
    "summary(glm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{Pr}(default=yes|student=yes) = \\frac{e^{-3.5+0.4049*1}}{1+e^{-3.5+0.4049*1}} = 0.043$$\n",
    "$$\\hat{Pr}(default=yes|student=no) = \\frac{e^{-3.5+0.4049*0}}{1+e^{-3.5+0.4049*0}} = 0.0292$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Logistic Regression\n",
    "\n",
    "$$p(X) = Pr(Y = 1|X) = \\frac{e^{\\beta_0 + \\beta_1X_1+...+\\beta_pX_p}} {1+e^{\\beta_0 + \\beta_1X_1+...+\\beta_pX_p}}$$\n",
    "\n",
    "$$log(\\frac{p(X)}{1 - p(X)}) = \\beta_0 + \\beta_1X_1+...+\\beta_pX_p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = default ~ balance + income + student, family = binomial, \n",
       "    data = Default)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.4691  -0.1418  -0.0557  -0.0203   3.7383  \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***\n",
       "balance      5.737e-03  2.319e-04  24.738  < 2e-16 ***\n",
       "income       3.033e-06  8.203e-06   0.370  0.71152    \n",
       "studentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 2920.6  on 9999  degrees of freedom\n",
       "Residual deviance: 1571.5  on 9996  degrees of freedom\n",
       "AIC: 1579.5\n",
       "\n",
       "Number of Fisher Scoring iterations: 8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "glm.fit = glm.fit=glm(default~balance+income+student,data=Default,family=binomial)\n",
    "summary(glm.fit)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Confounding\n",
    "  \n",
    "* Coefficient for studentYes is negative but when ran just the single predictor student the coefficient was positive\n",
    "* Negative coefficient indicates that a student is less likely to default than a non-student, positive indicates more likely \n",
    "* For a fixed balance and income students are less likely to default,\n",
    "    however, averaged over balance and income students are more likely to default (see left figure below)\n",
    "* Student and balance are correlated.\n",
    "    - Students tend to have higher balances and higher balances tend to default more\n",
    "    - Therefore, overall, students tend to default at a higher rate\n",
    "* If don't know balance, students are riskier, if know balance, then students are less riskier (than a non-student with the same balance)\n",
    "    - Confounding: results obtained using 1 predictor may be different than results obtained with multiple predictors, especially if there are correlations among predictors\n",
    " \n",
    "![](confounding.png)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Species ~ Petal.Length + Petal.Width, family = binomial, \n",
       "    data = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "     Min        1Q    Median        3Q       Max  \n",
       "-1.82880  -0.04096  -0.00014   0.01585   1.77240  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error z value Pr(>|z|)   \n",
       "(Intercept)   -45.483     15.808  -2.877  0.00401 **\n",
       "Petal.Length    5.810      2.575   2.256  0.02406 * \n",
       "Petal.Width    10.590      4.446   2.382  0.01722 * \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 110.904  on 79  degrees of freedom\n",
       "Residual deviance:  16.499  on 77  degrees of freedom\n",
       "AIC: 22.499\n",
       "\n",
       "Number of Fisher Scoring iterations: 9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>virginica</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>versicolor</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>virginica</th><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & virginica\\\\\n",
       "\\hline\n",
       "\tversicolor & 0\\\\\n",
       "\tvirginica & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | virginica | \n",
       "|---|---|\n",
       "| versicolor | 0 | \n",
       "| virginica | 1 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "           virginica\n",
       "versicolor 0        \n",
       "virginica  1        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Test Error:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.05"
      ],
      "text/latex": [
       "0.05"
      ],
      "text/markdown": [
       "0.05"
      ],
      "text/plain": [
       "[1] 0.05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Confusion Matrix:\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            predict.preds\n",
       "test.preds   versicolor virginica\n",
       "  versicolor          9         1\n",
       "  virginica           0        10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Test Error:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.05"
      ],
      "text/latex": [
       "0.05"
      ],
      "text/markdown": [
       "0.05"
      ],
      "text/plain": [
       "[1] 0.05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caTools)\n",
    "set.seed(1234)\n",
    "df <- read.csv(\"lr_example.csv\")\n",
    "\n",
    "split = sample.split(df$Species, SplitRatio = 0.8)\n",
    "train = subset(df, split == TRUE)\n",
    "test = subset(df, split == FALSE)\n",
    "\n",
    "glm.fit = glm(Species~Petal.Length+Petal.Width,data=train,family=binomial)\n",
    "glm.sum = summary(glm.fit)\n",
    "glm.sum\n",
    "contrasts(train$Species)\n",
    "predict.preds = predict(glm.fit,test,type=\"response\")\n",
    "predict.preds = ifelse (predict.preds > .5, \"virginica\", \"versicolor\")\n",
    "test.preds = test$Species\n",
    "test_error = mean(predict.preds != test.preds)\n",
    "print(\"Test Error:\")\n",
    "test_error\n",
    "cm = table(test.preds,predict.preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm\n",
    "test_error = 1 - (sum(diag(cm))/sum(cm))\n",
    "print(\"Test Error:\")\n",
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0]\n",
      " [ 1  9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schoem/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.050000000000000044"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('lr_example.csv')\n",
    "X = dataset.iloc[:, [3, 4]].values\n",
    "y = dataset.iloc[:, 5].values\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
    "\n",
    "# Fit Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(random_state = 1234)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "# Predict the Test set results\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "\n",
    "# Make the Confusion Matrix and calculate the test error rate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "test_error = 1 - (np.trace(cm)/np.sum(cm))\n",
    "test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data by indexing\n",
    " \n",
    "* In R the split function splits data into groups by a factor\n",
    "* When you don't have a factor, split using indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "122"
      ],
      "text/latex": [
       "122"
      ],
      "text/markdown": [
       "122"
      ],
      "text/plain": [
       "[1] 122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "31"
      ],
      "text/latex": [
       "31"
      ],
      "text/markdown": [
       "31"
      ],
      "text/plain": [
       "[1] 31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = .8 * nrow(airquality)\n",
    "indxs = sample(1:n,n) #permute 1-n\n",
    "train = airquality[indxs,]\n",
    "test = airquality[-indxs,]\n",
    "nrow(train)\n",
    "nrow(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Some of the figures in this presentation are taken from \"An Introduction to Statistical Learning, with applications in R\"  (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani \" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
