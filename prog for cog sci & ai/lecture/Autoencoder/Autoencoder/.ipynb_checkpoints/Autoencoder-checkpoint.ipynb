{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e433604-389c-43f8-bd98-6af87d4a73a1",
   "metadata": {
    "colab_type": "text",
    "id": "TM83wQpTZWNL"
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "Autoencoders are self-supervised techniques that reconstructs the input using a hidden layer with fewer active units than the number of input units. They are self-supervised since they use the input as the labels.\n",
    "\n",
    "![](Basic_Autoencoder.png)\n",
    "\n",
    "The middle layer is a bottleneck. In the figure above 4 input features are transformed into two latent features.\n",
    "\n",
    "Autoencoders try to determine what are the most important features of the input. They are data-specific, i.e. They are only be able to compress data similar to what they have been trained on.  They are Lossy, i.e. the outputs will be degraded compared to the original inputs.  \n",
    "\n",
    "    \n",
    "    \n",
    "#### Mapping\n",
    "\n",
    "$$\\text{Encoder: }\\Phi: \\mathcal{X} -> \\mathcal{Z} \\\\\n",
    "  \\text{Decoder: }\\Psi: \\mathcal{Z} -> \\mathcal{X} \\\\\n",
    "  \\Phi,\\Psi =  \\underset{\\Phi,\\Psi}{\\mathrm{argmin}}||X - (\\Psi\\circ\\Phi)X||^2$$\n",
    "\n",
    "The Encoder $\\Phi$ maps input to latent space $\\mathcal{Z}$, the bottleneck. $a_E$ is the activation function.\n",
    "$$z = a_E(Wx + b)$$\n",
    "\n",
    "The Decoder $\\Psi$ maps latent space $\\mathcal{Z}$ to the output which is the same as the input. $a_D$ is the activation function.\n",
    "$$x^{'} = a_D(Uz + c)$$\n",
    "\n",
    "\n",
    "#### Types of Autoencoders\n",
    "\n",
    "* Basic (shown above)\n",
    "* Sparse\n",
    "* Denoising\n",
    "* Stacked/Deep\n",
    "* Convolutional\n",
    "* Variational\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b610bc-2144-4e08-8398-1d7645d86099",
   "metadata": {
    "colab_type": "text",
    "id": "o6yfq0ajZWN8"
   },
   "source": [
    "### Sparse Autoencoder\n",
    "\n",
    "A Sparse Autoencoder usually has more hidden units than input units but only some of the hidden unit are active during any feed forward operation. They are used for feature extraction.\n",
    "\n",
    "Sparsity constraints are a type of regularization and can be imposed through L1 regularization or KL-divergence.\n",
    "\n",
    "![](Sparse_Autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad4f73-7e96-4888-bd98-0a2e4576fd30",
   "metadata": {
    "colab_type": "text",
    "id": "dtjWEAuiZWON"
   },
   "source": [
    "### Denoising Autoencoder\n",
    "\n",
    "A Denoising Autoencoder reconstructs data from an input of corrupted data. This prevents just \"memorizing\" the input. Inputs are randomly removed (i.e. set to zero).\n",
    "\n",
    "\n",
    "![](Denoising.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b54b51-6105-4e66-8a4b-4a46f27df98e",
   "metadata": {
    "colab_type": "text",
    "id": "EbYqrT0HZWOa"
   },
   "source": [
    "#### Stacked/Deep Autoencoder\n",
    "\n",
    "A Stacked/Deep Autoencoder has more than 1 hidden layer.\n",
    "\n",
    "![](Stacked.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387ddf4-061c-46d5-a1c2-bd4582112f1b",
   "metadata": {
    "colab_type": "text",
    "id": "ZaCCBMYgZWOg"
   },
   "source": [
    "### Convolutional Autoencoder\n",
    "\n",
    "A Convulutional Autoencoder Tties to reconstruct the input image by replacing the fully-connected layer of a CNN. It learns to remove noise from image and reconstruct missing parts.\n",
    "\n",
    "![](ConvolAutoEncode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe9add-2377-4fbf-9f0c-f571559f1511",
   "metadata": {},
   "source": [
    "## Variational Autoencoder\n",
    "\n",
    "\n",
    "The encoder produces a probability distribution for each latent variable rather than a single value.\n",
    "\n",
    "![](VAE_distrib.png)\n",
    "\n",
    "$\\text{Latent Variable Distributions. Source: Jeremy Jordan}$\n",
    "\n",
    "The decoder samples from these distribution to reconstruct the original image. The objective is to make the  reconstruction similar to the original image. The degree of similarity is determined by an information theoretic measure called Kullback-Leiber(KL)-Divergence which is determined by Variational Inference, an optimization method that approximates probability densities. \n",
    "\n",
    "A variational autoencoder learns smooth latent state representations of the input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07437fd8-73e2-4ad4-a275-7561467c6d1e",
   "metadata": {},
   "source": [
    "#### Variational Inference\n",
    "\n",
    "Given: Observed variables **x** and hidden or latent variables **z** we wish to infer p(**z**|**x**)\n",
    "\n",
    "![](VarInfer.png)\n",
    "\n",
    "$\\text{Variational Inferrence. Source: David Blei}$\n",
    "\n",
    "To do this, we create a **variational distribution** over the hidden variables $q(z_{1:m}|\\nu)$ then find settings of $\\nu$ such that q is close to p. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996b823-1f92-4014-9c88-56ab50b3ea4f",
   "metadata": {},
   "source": [
    "### Kullback-Leibler (KL) Divergence (Relative Entropy)\n",
    "\n",
    "KL is a measure of the \"distance\" between two distributions. It measures the similarity of two distributions with Entropy as the measure. (it can't be a distance measure because $KL(q||p) \\ne KL(p||q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bb8e1-5c04-428c-bca1-91f62f0fef0b",
   "metadata": {},
   "source": [
    "**Information**: The reduction in uncertainty derived from learning an outcome, quantified by  minus the log probability of an event\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ I = -log_2P(x)$$\n",
    "</div>\n",
    "\n",
    "x is an event, e.g. a coin flip came up heads\n",
    "\n",
    "\n",
    "Probability $\\uparrow$ Information $\\downarrow$\n",
    "\n",
    "\n",
    "**Information Entropy**: Average Information\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ Entropy = H = -E(I) = -\\sum_i p(x_i)*log(p(x_i))$$\n",
    "</div>\n",
    "\n",
    "\n",
    "**Differential Entropy**: Entropy of Continuous Random Variables.\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$Entropy = H = -\\int{p(x)*log(p(x)} dx$$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84feb4db-78ce-41c7-957b-fcbd5e4614d7",
   "metadata": {},
   "source": [
    "#### KL example for discrete distribution\n",
    "\n",
    "Suppose you have 3 4-sided die with these PMFs:\n",
    "\n",
    "* Dice A: p(x): (.25, .25, .25, .25)\n",
    "* Dice B: q(x): (.25, .26, .24, .25)\n",
    "* Dice C: r(x): (.4, .15, .05, .4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6057a249-58fb-413e-acc5-53a3eaf97568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "p = (.25, .25, .25, .25)\n",
    "q = (.25, .26, .24, .25)\n",
    "r = (.4, .15, .05, .4)\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(12,6))\n",
    "ax1.vlines(x,0,p)\n",
    "ax1.set_title(\"PMF for p\")\n",
    "ax2.vlines(x,0,q)\n",
    "ax2.set_title(\"PMF for q\")\n",
    "ax3.vlines(x,0,r)\n",
    "ax3.set_title(\"PMF for r\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4293bb-454b-449f-8269-a811e882db57",
   "metadata": {
    "tags": []
   },
   "source": [
    "   \n",
    "KL(p||q) should be small and KL(p||r) should be much larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9b39f6-8191-4e16-9009-a8cd5a8fff8a",
   "metadata": {
    "id": "PUJszG6X-IjC",
    "tags": []
   },
   "source": [
    "#### KL divergence example for continuous distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e927cc-bd32-45e2-b918-02f19ac25425",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OEM2xJ39PUZ",
    "outputId": "b0011525-77d2-4c20-b708-17086bac1913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: -3.810887336730957\n",
      "log prob pz: -8.18036937713623, prob: 0.0002800984657369554\n",
      "log prob qzx: -3.3604331016540527, prob: 0.03472021967172623\n",
      "kl_divergence: 4.819936275482178\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "p = torch.distributions.Normal(0, 1)\n",
    "q = torch.distributions.Normal(2, 4)\n",
    "\n",
    "z = q.rsample()\n",
    "print(f'z: {z}')\n",
    "\n",
    "log_pz = p.log_prob(z)\n",
    "log_qzx = q.log_prob(z)\n",
    "\n",
    "print(f'log prob pz: {log_pz}, prob: {torch.exp(log_pz)}')\n",
    "print(f'log prob qzx: {log_qzx}, prob: {torch.exp(log_qzx)}')\n",
    "\n",
    "kl_divergence = log_qzx - log_pz\n",
    "print(f'kl_divergence: {kl_divergence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2010b-a5c2-4931-bd4d-e4b396dea7ce",
   "metadata": {
    "id": "q_N9da8V-Qom"
   },
   "source": [
    "#### Move move q closer to p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e55e14-a14d-4f7d-a4d5-83ee6e603caa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q59dzIZq-eSG",
    "outputId": "da836119-546a-443e-93b3-7ea65e74138f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob qzx: -4.505165100097656, prob: 0.011051765643060207\n",
      "New kl_divergence: 3.675204277038574, Difference -1.1447319984436035\n"
     ]
    }
   ],
   "source": [
    "q = torch.distributions.Normal(1, 2)\n",
    "\n",
    "log_qzx = q.log_prob(z)\n",
    "print(f'log prob qzx: {log_qzx}, prob: {torch.exp(log_qzx)}')\n",
    "\n",
    "new_kl_divergence = log_qzx - log_pz\n",
    "print(f'New kl_divergence: {new_kl_divergence}, Difference {new_kl_divergence - kl_divergence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ae924-335e-4d41-b648-0241fe7db1be",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### KL-Divergence Definition\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$KL(p||q) = \\sum p(x) log(p(x)) - \\sum p(x) log(q(x)) \\\\\n",
    "            = \\sum p(x)[log(p(x)) - log(q(x))] \\\\\n",
    "            = \\sum p(x) \\left [log\\frac{p(x)}{q(x)} \\right ] = - \\sum p(x) \\left [log\\frac{q(x)}{p(x)} \\right]$$\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdeef58-24ba-42b0-a009-0954b4149063",
   "metadata": {},
   "source": [
    "#### Properties of KL Divergence\n",
    "\n",
    "* KL Divergence >= 0, 0 when p(x) = q(x)\n",
    "* Asymmetric: KL(p||q) $\\ne$ KL(q||p). Note: Not really a distance measure since KL(p||q) $\\ne$ KL(q||p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbc133-408a-4f20-92cc-b9a2eadd2ee6",
   "metadata": {},
   "source": [
    "### Minimizing the distance between q(z) and p(z|x)\n",
    "\n",
    "We use q(z) to estimate p(z|x). We wnat to minimize the difference between the target p(z|x) and the proposal q(z), i.e. minimize KL(q(z)||p(z|x))\n",
    "\n",
    "It is hard to compute p(z|x), but remember $p(z|x) = \\frac{p(z,x)}{p(x)}$.\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "   \n",
    "\n",
    "$$KL(q(z)||p(z|x)) = -\\sum q(z) log \\frac{p(z|x)}{q(z)}$$\n",
    "\n",
    "$$p(z|x) = \\frac{p(z,x)}{p(x)} $$\n",
    "\n",
    "$$KL = -\\sum q(z) log \\frac{p(x,z)}{p(x)}\\frac{1}{q(z)} $$\n",
    "$$KL = -\\sum q(z) log\\frac{p(x,z)}{q(z)}\\frac{1}{p(x)} $$\n",
    "$$KL = -\\sum q(z) [log \\frac{p(x,z)}{q(z)} + log \\frac{1}{p(x)}] $$\n",
    "$$KL = -\\sum q(z) [log \\frac{p(x,z)}{q(z)} - log( p(x))] $$\n",
    "$$KL = -\\sum q(z)log \\frac{p(x,z)}{q(z)} + \\sum q(z) log(p(x)) $$\n",
    "$$KL + \\sum q(z)log \\frac{p(x,z)}{q(z)}  = log(p(x))$$\n",
    "$$KL + \\sum q(z)log (p(x,z)) - \\sum q(z) log(q(z)) = log(p(x)$$\n",
    "$$KL + L(x) = log(p(x))$$\n",
    "</div>\n",
    "\n",
    "Note:\n",
    "* log (p(x)) < 0 since $0 \\le p(x) \\le 1$, and log(1) = 0\n",
    "* log (p(x)) is fixed, i.e. a constant\n",
    "\n",
    "so KL divergence is bounded and nonnegative.\n",
    "\n",
    "Since KL + L equals a constant, if one increases the other must decrease. L(X) is called the **Evidence Lower Bound (ELBO)**. To minimize KL, maximize L, the ELBO.\n",
    "\n",
    "#### Evidence Lower Bound (ELBO)\n",
    "\n",
    "$$L(x) = \\sum q(z)log( p(x,z)) - \\sum q(z) log (q(z))$$\n",
    "\n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$L(x) =  \\sum q(z)log (p(x,z)/q(z))  = \\\\\n",
    " \\sum q(z)[log p(x|z)p(z)/q(z)]  = \\\\\n",
    "\\sum q(z)[log (p(x|z)) + log p(z)/Q(z)] = \\\\\n",
    "\\sum q(z)log (p(x|z) + \\sum q(z)log p(x|z)/q(z) = \\\\\n",
    "\\mathbb{E}_q log p(x|z) - KL(q(z)||p(x))$$\n",
    "</div>\n",
    "\n",
    "The first term is the expectation of the likelihood of x and the second term is the similarity of q(z) to p(x).\n",
    "\n",
    "This is the objective function for training a VAE.\n",
    "\n",
    "### VAE Latent Space Representation\n",
    "\n",
    "![](VAE_Mapping.png)\n",
    "\n",
    "$\\text{Encoder, Decoder Distributions. Source: Jeremy Jordan}$\n",
    "\n",
    "q(z|x) maps x to z, p(x|z) maps z to x.\n",
    "\n",
    "#### Assumption: The latent variables have Normal distributions with parameters mean and variance\n",
    "\n",
    "The encoder produces a mean and variance for each Normally distributed latent variable.\n",
    "\n",
    "The decoder can sample from these Normal random variables to reconstruct the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d2605-d09d-4c21-85f2-2201431357c6",
   "metadata": {},
   "source": [
    "![](VAE.png)\n",
    "\n",
    "$\\text{Variational Autoencoder. Source: Jeremy Jordan}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23296579-80ed-4301-afdc-d2e0fb767620",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Training with backpropagation can't be implemented through the random sampling step in the algorithm. However there is a trick called 'Reparameterization' that allows backpropagation through the node.\n",
    "\n",
    "![](Reparm.png)\n",
    "\n",
    "$\\text{Reparameterization. Source: Jeremy Jordan}$\n",
    "\n",
    "Reparameterization uses the fact that any Normal distribution can be generated by shifting and scaling a standard Normal distribution. \n",
    "\n",
    "With the reparameterization backprop can be executed on the deterministic node and still allow sampling to occur.\n",
    "\n",
    "![](Reparm2.png)\n",
    "\n",
    "$\\text{Backpropagation. Source: Jeremy Jordan}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad16d2-1fff-4b26-8dac-a7f7826b071d",
   "metadata": {
    "colab_type": "text",
    "id": "qZwO4wTvZWO1"
   },
   "source": [
    "### References\n",
    "\n",
    "https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n",
    "\n",
    "https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "https://machinelearningmastery.com/lstm-autoencoders/\n",
    "\n",
    "https://analyticsindiamag.com/how-to-implement-convolutional-autoencoder-in-pytorch-with-cuda/\n",
    "\n",
    "https://www.jeremyjordan.me/variational-autoencoders/\n",
    "\n",
    "https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "\n",
    "https://arxiv.org/abs/1601.00670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107d4d7-0135-4ac7-8b95-bdbd0cdc6183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
