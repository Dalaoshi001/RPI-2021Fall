{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Discriminative and Generative Models\n",
    "\n",
    "#### Discriminative Models\n",
    "\n",
    "Logistic regression, multivariate logistic and softmax regression are discriminative models. They all directly compute the conditional probability P(y|x), i.e. the probability of a given class knowing the feature x.\n",
    "\n",
    "Discriminative models determine the boundary between classes. They classify points without providing a model of how the points are actually generated. They directly model the mapping from the independent variables to the dependent ones.\n",
    "\n",
    "Classification is done using a threshold to turn the computed probability into a boundary that determines class assignment\n",
    "\n",
    "Discriminative models don't make as many assumptions as generative models.\n",
    "\n",
    "#### Generative Models\n",
    "\n",
    "Naive Bayes and Linear Discriminant Analysis (LDA) are Generative Models. They first model P(x|y) i.e. they model the distribution of x for each class and then assign the class label. \n",
    "\n",
    "They compute the joint distribution, P(x,y) = P(x|y)P(y). This models how the data was generated and enables generation of samples for each class.\n",
    "\n",
    "Generative models make assumptions about structure of data. For example, the independence assumption in Naive Bayes and the distribution of the classes (P(y)) in LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "A family of classification algorithms based on Bayes' Theorem\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$P(Class|Features) = \\frac{P(Class)P(Features|Class)}{P(Features)}$$\n",
    "</div>\n",
    "\n",
    "They all assume that the features (i.e. predictors) are independent, the \"Naive\" assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    forecast  temp humidity    wind Play\n",
       "0      Sunny   Hot     High    Weak   No\n",
       "1      Sunny   Hot     High  Strong   No\n",
       "2   Overcast   Hot     High    Weak  Yes\n",
       "3       Rain  Mild     High    Weak  Yes\n",
       "4       Rain  Cool   Normal    Weak  Yes\n",
       "5       Rain  Cool   Normal  Strong   No\n",
       "6   Overcast  Cool   Normal  Strong  Yes\n",
       "7      Sunny  Mild     High    Weak   No\n",
       "8      Sunny  Cool   Normal    Weak  Yes\n",
       "9       Rain  Mild   Normal    Weak  Yes\n",
       "10     Sunny  Mild   Normal  Strong  Yes\n",
       "11  Overcast  Mild     High  Strong  Yes\n",
       "12  Overcast   Hot   Normal    Weak  Yes\n",
       "13      Rain  Mild     High  Strong   No"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golf = pd.read_csv(\"golf.csv\",quotechar=\"'\",escapechar=\"/\")\n",
    "golf.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golf dataset\n",
    "\n",
    "The data set is a matrix of features containing 14 observations of 4 features: forecast,temp, humidity, and wind. The Response is a vector of two Contains value of two classes or categories: Play=yes and Play = no.\n",
    "\n",
    "The goal is to predict whether to play golf given a set of weather conditions. For today's data, D = (forecast,temp,humidity and wind), which is more probable, P(Play = yes|D) or P(Play = no|D)\n",
    "\n",
    "We assume the features are independent and are equally important in predicting Play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Probabilities\n",
    "\n",
    "The class probabilities P(C) are the count number of 'yes's and 'No's and divided by the total number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes    9\n",
      "No     5\n",
      "Name: Play, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.64, 0.36)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnts = golf.Play.value_counts()\n",
    "print(cnts)\n",
    "L = np.sum(cnts)\n",
    "\n",
    "P_yes = cnts['Yes']/L\n",
    "P_no = cnts['No']/L\n",
    "P_yes.round(2),P_no.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Feature Probabilities P(feature = value | Class = value)\n",
    "\n",
    "The likelihood of the feature given the class is the count of the feature when the class occurred divided the total number of instances of the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_feat_prob (feat,val,cls='Yes'):\n",
    "    return((sum((golf[feat] == val) & (golf.Play == cls))/cnts[cls]).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.67, 0.33, 0.2, 0.8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Humudity\n",
    "P_normal_yes = cond_feat_prob('humidity','Normal')\n",
    "P_high_yes = cond_feat_prob('humidity','High')\n",
    "P_normal_no = cond_feat_prob('humidity','Normal',cls='No')\n",
    "P_high_no = cond_feat_prob('humidity','High',cls='No')\n",
    "P_normal_yes,P_high_yes,P_normal_no,P_high_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.67, 0.33, 0.4, 0.6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wind\n",
    "P_weak_yes = cond_feat_prob('wind','Weak')\n",
    "P_strong_yes = cond_feat_prob('wind','Strong')\n",
    "P_weak_no = cond_feat_prob('wind','Weak',cls='No')\n",
    "P_strong_no = cond_feat_prob('wind','Strong',cls='No')\n",
    "P_weak_yes,P_strong_yes,P_weak_no,P_strong_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Play today = yes  given today's  features:  P(Play=yes|X) ~ P(X|Play=yes) * P(Play=yes)\n",
    "\n",
    "Probability that you **will** play today is proportional to the Likelihood of the features times the Prior probability that you will play based on your past experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = ('High','Strong') # Today has High Humidity and Strong Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_yes_today = (P_high_yes * P_strong_yes) * P_yes # Independence Assumption\n",
    "P_yes_today.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Play today = no  given today's  features: P(Play=no | X) ~ P(X|Play=no) * P(Play = no)\n",
    "\n",
    "Probability that you **wont** play today is proportional to the Likelihood of the features times the Prior probability that you will play based on your past experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.171"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_no_today = (P_high_no * P_strong_no) * P_no #Independence Assumption\n",
    "P_no_today.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play?\n",
    "\n",
    "Compare P_yes_features to P_no_features. You don't need to compute P(Features) to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study\n"
     ]
    }
   ],
   "source": [
    "print('Play') if (P_yes_today / P_no_today) > 1 else print(\"Study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to probabilities by normalizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2899618354486554, 0.7100381645513446)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_today = P_yes_today + P_no_today \n",
    "\n",
    "P_yes_X = P_yes_today / P_today\n",
    "P_no_X = P_no_today / P_today\n",
    "\n",
    "P_yes_X,P_no_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier\n",
    " \n",
    "\n",
    "A Bayes Classifier assigns to each observation the most likely class given its feature vector. \n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ \\arg\\max_{y} Pr(Y = y|X = x)\\text{, }x \\in R^d\\text{, }y\\in \\{1,2,3,..,m\\}$$\n",
    "</div>\n",
    "\n",
    "Naive Bayes is a special case of a Bayes Classifier since it assumes the features are independent.\n",
    "\n",
    "#### Test Error Rate for Classification:\n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$\\frac{1}{n}\\sum^n_{i=1}I(y_i \\neq \\hat{y}_i)$$  \n",
    "</div>\n",
    "\n",
    "$I(y_i \\neq \\hat{y}_i)$ is an indicator variable = 1 if $y_i \\neq \\hat{y}_i$ and 0 if $y_i = \\hat{y}_i$\n",
    "\n",
    "It can be proven that the Bayes Classifier produces the lowest possible test error rate, called the Bayes Error Rate.\n",
    "\n",
    "<div style=\"font-size: 105%;\">\n",
    "$$ BayesErrorRate = 1 - E(\\arg\\max_{y}(Pr(Y=y|X)))$$\n",
    "</dev>\n",
    "\n",
    "The Expectation (E) averages the probability over all possible values of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### How to calculate P(Y=Class|X = features) ??\n",
    "\n",
    "In general you can't compute the posterior conditional distribution because you have on the order of $2^n$ parameters if you have n binary features. Therefore you need to make some assumptions.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "#### Naive Bayes Assumptions:\n",
    " \n",
    "1) All features are of equal importance.   \n",
    "2) All features are conditionally independent.\n",
    "\n",
    "The second assumption is often violated, since the features are often somewhat correlated. Even so, it works surprisingly well in many cases. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Probability\n",
    " \n",
    "What is the probability that a particular object belongs to class y given its observed feature values?\n",
    "\n",
    "Given:\n",
    "\n",
    "y is the class variable  \n",
    "$x_i$ represents ith feature in the feature vector \n",
    "\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "\n",
    "$$P(y|x_1,...,x_d) = \\frac{P(y)P(x_1,...,x_d | y)}{P(x_1,...,x_d)}$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence Assumption\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_d) = P(x_i|y)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Independence Assumption\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$P(y|x_1,...,x_d) = \\frac{P(y)\\prod^d_{i=1}P(x_i|y)}{P(x_1,...,x_d)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(x_1,...,x_d)$ doesn't depend on y (it is a constant), so\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$P(y|x_1,...,x_d) \\propto{P(y)\\prod^d_{i=1}P(x_i|y)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Rule\n",
    " \n",
    "Maximize the posterior probability given the training data to formulate a decision rule for new data.\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$\\hat{y} = \\arg\\max_{y} P(y)\\prod^d_{i=1}P(x_i|y)$$\n",
    "</div>\n",
    "\n",
    "#### Estimate the prior P(y) and the Likelihood $P(x_i|y)$ from the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Prior Probability P(y)\n",
    " \n",
    "The prior probability is the probability that new observation belongs to class y. It is based on expert knowledge or estimated from training data (assumes traing data is i.d.d.).\n",
    "\n",
    "The Maximum Likelihood Estimate is the relative frequency of class y in the training data.\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ \\hat{P}(Y = y) = \\frac{NumY}{NumTotal}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the  Likelihoods $P(x_i|y)$\n",
    "\n",
    "Because of the independence assumption, each feature ($x_i$) can have its own distribution. The feature can be Categorical or Numerical.\n",
    "\n",
    "#### Likelihood for Categorical data\n",
    "\n",
    "The likelihoods for every feature is estimated to be simply the frequency (categorical data).\n",
    "\n",
    "The Maximum Likelihood Estimate is:\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$\\hat{P}(X_i = x|Y=y) = \\frac{NumJoint(x,y)}{NumTotal(Y=y)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood for Gaussian numerical data\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$\\hat{P}(x_i|y) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn Naive Bayes\n",
    "\n",
    "sklearn has Naive Bayes models for the following likelihoods:\n",
    "\n",
    "* Gaussian: continuous features\n",
    "* Multinomial: discrete features (generalization of binomial)\n",
    "* Bernoulli: discrete features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Gaussian Naive Bayes\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  EstimatedSalary  Purchased\n",
       "0   19            19000          0\n",
       "1   35            20000          0\n",
       "2   26            43000          0\n",
       "3   27            57000          0\n",
       "4   19            76000          0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "df = pd.read_csv('KNN_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   19, 19000],\n",
       "        [   35, 20000],\n",
       "        [   26, 43000],\n",
       "        [   27, 57000],\n",
       "        [   19, 76000]], dtype=int64),\n",
       " array([0, 0, 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, [0, 1]].values\n",
    "y = df.iloc[:, 2].values\n",
    "X[0:5,:],y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 2), (100, 2), (300,), (100,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into the training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,\n",
    "                                                    random_state = 1234,stratify= y)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Age and Salary features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Naive Bayes model to the training data\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the test data\n",
    "preds = model.predict(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61  3]\n",
      " [10 26]]\n",
      "Accuracy:  0.87\n"
     ]
    }
   ],
   "source": [
    "# Make  the Confusion Matrix and compute the accuracy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(cm)\n",
    "accuracy = np.trace(cm)/np.sum(cm)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap([\"white\", \"black\"])\n",
    "def plot_decision_boundary(model,X_data,y_data,ax):\n",
    "    # Make grid: min to max of 1st column, min to max of 2nd column in small increments\n",
    "    X1,X2 = np.meshgrid(np.arange(X_data[:, 0].min() - 1, X_data[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(X_data[:, 1].min() - 1, X_data[:, 1].max() + 1, step = 0.01))\n",
    "    # Flatten X1 and X2, create an array and transpose to a 2-column array (one for each feature)\n",
    "    v = np.array([X1.ravel(), X2.ravel()]).T\n",
    "    #  Using fitted model, predict the points in v and reshape to a 2-dimensional array\n",
    "    ax.contourf(X1, X2, model.predict(v).reshape(X1.shape),alpha = 0.75,cmap = ListedColormap(('magenta','cyan')))\n",
    "    ax.set_xlim(X1.min(), X1.max())\n",
    "    ax.set_ylim(X2.min(), X2.max())\n",
    "    for i, j in enumerate(np.unique(y_data)): # For each class\n",
    "        ax.scatter(X_data[y_data == j, 0], X_data[y_data == j, 1],\n",
    "                label = j,\n",
    "                c = np.array(ListedColormap(('white', 'black'))(i)).reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize = (10,6))\n",
    "\n",
    "# Visualising the Training set results\n",
    "plot_decision_boundary(model,X_train,y_train,ax1)\n",
    "ax1.set_title('Training Data')\n",
    "ax1.set_xlabel('Age')\n",
    "ax1.set_ylabel('Salary')\n",
    "\n",
    "\n",
    "# Visualising the Test set results\n",
    "plot_decision_boundary(model,X_test,y_test,ax2)\n",
    "ax2.set_title('Test Data')\n",
    "ax2.set_xlabel('Age')\n",
    "ax2.set_ylabel('Salary');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Distributed Data\n",
    "\n",
    "Classification with discrete features, e.g. word counts for text classification. \n",
    "\n",
    "Multinomial distribution: extends binomial to more than 2 categories. Parameter n is the number of trials. Parameter p is a vector of probabilities, one per category. Estimated by additive smoothing.\n",
    "\n",
    "Used in Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Smoothing\n",
    "\n",
    "In testing, it is possible to encounter a feature not seen in training. The conditional probability for that feature will be 0 making the class conditional probability 0.\n",
    "\n",
    "To ensure conditional probability not 0 by adding an additional smoothing term. \n",
    "    \n",
    "<div style=\"font-size: 115%;\">\n",
    "$$\\hat{P}(x_i|y_j) = \\frac{N_{x_i,y_j} + \\alpha}{Ny_j + \\alpha d}\\text{ }\\forall{i} = 1,2,...d$$\n",
    "</div>\n",
    "\n",
    "* d is the number of features\n",
    "* $\\alpha$ is a parameter for additive smoothing  \n",
    "    - Laplace Smoothing: $\\alpha$ = 1  \n",
    "    - Lidstone Smoothing: $\\alpha$ < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Multinomial Naive Bayes\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate some data: counts of words in documents\n",
    "\n",
    "The rows are topics and the columns are words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "X = np.random.randint(5, size=(6, 100)) #5 = counts, 6 = # of observations, 100 = # of features\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "print(X.shape)\n",
    "print(np.unique(X,return_counts=True))\n",
    "X[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha is the Laplace/Lidstone smoothing parameter, default = Laplace\n",
    "model = MultinomialNB(alpha = 1.0) \n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict class of features X[2] and probabilities of each class \n",
    "\n",
    "Truth is topic = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(X[2:3]))\n",
    "\n",
    "model.predict_proba(X[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernouli Distributed Data\n",
    "\n",
    "Data is distributed according to multivariate Bernoulli distributions. There may be multiple features but each one is assumed to be a boolean variable. \n",
    "\n",
    "Used in Natural Language Processing for Text Classification.\n",
    "    \n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ \\hat{P}(x_i|y) = P(i|y)x_i + (1 - P(i|y))(1 - x_i)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Bernoulli Naive Bayes\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "X = np.random.randint(2, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "print(X.shape)\n",
    "print(np.unique(X,return_counts=True))\n",
    "X[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha is the Laplace/Lidstone smoothing parameter, default = Laplace\n",
    "model = BernoulliNB(alpha = 1.0) \n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X[2].reshape(1,-1);X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict class of features (X[2] and probabilities of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(X2))\n",
    "model.predict_proba(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Pros and Cons\n",
    "\n",
    "#### Pros\n",
    "\n",
    "* Easy and fast to predict classes of the test set\n",
    "* Can perform better than other algorithms assuming independence\n",
    "* Performs well with categorical input\n",
    "\n",
    "#### Cons\n",
    "\n",
    "* Test set can have features with zero frequency in training set\n",
    "* Not a good estimator of the probabilities\n",
    "* Independence assumption often violated \n",
    "\n",
    "### Why independence assumption?\n",
    "\n",
    "* Even though it is often a poor assumption, it simplifies the model\n",
    "\n",
    "#### A bad simple model often outperforms a better complex one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "505px",
    "left": "996px",
    "right": "20px",
    "top": "120px",
    "width": "284px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
