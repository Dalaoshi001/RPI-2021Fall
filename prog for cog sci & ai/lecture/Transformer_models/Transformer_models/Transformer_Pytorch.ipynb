{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yLGaJijqA7ab"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4Rf1b4E_A7ae"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEeoRVsopIQN"
   },
   "source": [
    "\n",
    "## Transformer for Sequence-to-Sequence Modeling \n",
    "\n",
    "A nn.TransformerEncoder model is trained on a language modeling task. The language modeling task is to assign a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words. \n",
    "\n",
    "\n",
    "https://pytorch.org/docs/master/generated/torch.nn.Transformer.html#torch.nn.Transformer\n",
    "\n",
    "nn.Transformer is based on the paper 'Attention is All You Need'\n",
    "https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "The nn.Transformer module relies on the attention mechanism nn.MultiheadAttention to draw global dependencies between input and output.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI2EzniMpIQN"
   },
   "source": [
    "### Model definition\n",
    "\n",
    "The nn.TransformerEncoder consists of multiple layers of nn.TransformerEncoderLayer.\n",
    "\n",
    "Along with the input sequence, a square attention mask is required because the self-attention layers in nn.TransformerEncoder are only allowed to attend the earlier positions in the sequence. For the language modeling task, any tokens on the future positions should be masked. To have the actual words, the output of nn.TransformerEncoder  model is sent to the final Linear layer, which is followed by a log-Softmax function.\n",
    "\n",
    "The Embedding layer:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EfRMB1U6lrHL"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        '''\n",
    "        ntoken:  size of vocabulary\n",
    "        d_model: embedding dimension\n",
    "        nhead: number of heads in nn.MultiheadAttention\n",
    "        d_hid: dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        nlayers: number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        dropout: dropout probability\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnWvw09mpIQP"
   },
   "source": [
    "#### Positional Encoding \n",
    "\n",
    "The PositionalEncoding module adds information about the positions of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed. \n",
    "\n",
    "The positions are indicated by different frequencies of the sine and cosine functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RL7iFhxlA7af"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXPkPahdpIQP"
   },
   "source": [
    "### Load and batch the  Wikitext-2 dataset \n",
    "\n",
    "The dataset input is the Wikitext-2 dataset from torchtext.   \n",
    "\n",
    "The vocab object is built based on the training dataset and is used to create numerical tokens as tensors. \n",
    "\n",
    "The batchify() function arranges the dataset into columns, trimming off any tokens remaining after the data has been divided into batches of size batch_size. For instance, with the alphabet as the sequence (total length of 26)and a batch size of 4, we would divide the alphabet into 4 sequences of length 6:\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "  \\end{bmatrix}\\end{align}\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of ``G`` and ``F`` can not be learned, but allows more efficient batch processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySPTVnTTA7ag",
    "outputId": "8c09a70b-b078-4753-eabc-8de33e1e3349"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.48M/4.48M [00:00<00:00, 15.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>']) \n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-ozYkWlpIQP"
   },
   "source": [
    "#### get_batch()\n",
    "\n",
    "This function generates the input and target sequence for the transformer model. It subdivides the source data into chunks of length bptt. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K0gSdZIXA7ah"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fYAtqutA7ai"
   },
   "source": [
    "### Instantiate the transformer model\n",
    "\n",
    "The model hyperparameters are defined below. The vocab size is\n",
    "equal to the length of the vocab object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H5x2KvBnA7ai"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHnA45s0A7ai"
   },
   "source": [
    "### Loss, Opitmizer and Learning Rate Scheduler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "riaZpCkUA7ai"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS01pRFWlrHO"
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "Save the model if the validation loss is the best we've seen so far. \n",
    "\n",
    "Adjust the learning rate after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_4QSwTbDlrHO"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwicA8fhlrHO"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fw5PMx5SlrHO"
   },
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJtN7qQmA7ai"
   },
   "source": [
    "Loop over epochs. Save the model if the validation loss is the best\n",
    "we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tejYh5eMA7ai",
    "outputId": "ef28f73d-8dbc-4757-d821-ab7d2da278a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 36.93 | loss  8.10 | ppl  3299.33\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 35.17 | loss  6.85 | ppl   940.69\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 35.29 | loss  6.44 | ppl   624.81\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 35.33 | loss  6.30 | ppl   543.82\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 35.40 | loss  6.19 | ppl   486.42\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 35.45 | loss  6.15 | ppl   466.82\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 35.61 | loss  6.11 | ppl   449.88\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 35.66 | loss  6.10 | ppl   447.93\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 35.62 | loss  6.02 | ppl   410.05\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 35.59 | loss  6.01 | ppl   409.25\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 35.53 | loss  5.89 | ppl   362.62\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 35.55 | loss  5.97 | ppl   391.47\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 35.59 | loss  5.95 | ppl   385.39\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 35.55 | loss  5.88 | ppl   358.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 107.94s | valid loss  5.79 | valid ppl   326.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 35.64 | loss  5.86 | ppl   350.47\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 35.67 | loss  5.84 | ppl   344.08\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 35.50 | loss  5.65 | ppl   285.58\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 35.52 | loss  5.69 | ppl   296.85\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 35.48 | loss  5.65 | ppl   284.23\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 35.54 | loss  5.68 | ppl   293.41\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 35.64 | loss  5.68 | ppl   293.70\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 35.53 | loss  5.71 | ppl   300.95\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 35.55 | loss  5.64 | ppl   282.14\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 35.50 | loss  5.67 | ppl   289.06\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 35.57 | loss  5.55 | ppl   256.10\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 35.50 | loss  5.64 | ppl   281.83\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 35.55 | loss  5.63 | ppl   279.79\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 35.63 | loss  5.57 | ppl   263.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 107.89s | valid loss  5.66 | valid ppl   286.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 35.75 | loss  5.60 | ppl   271.11\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 35.53 | loss  5.61 | ppl   272.86\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 35.62 | loss  5.42 | ppl   224.76\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 35.56 | loss  5.48 | ppl   239.79\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 35.47 | loss  5.43 | ppl   227.44\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 35.51 | loss  5.47 | ppl   238.49\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 35.52 | loss  5.48 | ppl   240.40\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 35.51 | loss  5.51 | ppl   247.84\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 35.54 | loss  5.48 | ppl   239.64\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 35.50 | loss  5.48 | ppl   240.37\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 35.45 | loss  5.36 | ppl   212.04\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 35.56 | loss  5.46 | ppl   235.77\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 35.48 | loss  5.47 | ppl   237.57\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 35.47 | loss  5.41 | ppl   222.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 107.77s | valid loss  5.59 | valid ppl   267.98\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tURR05O6A7ai"
   },
   "source": [
    "### Evaluate the best model on the test dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehkDmyTWA7ai",
    "outputId": "b68b97c6-080e-4763-f4b5-a88646aad57b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.50 | test ppl   244.88\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sNSH8pYlrHP"
   },
   "source": [
    "### Reference\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformer_Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
