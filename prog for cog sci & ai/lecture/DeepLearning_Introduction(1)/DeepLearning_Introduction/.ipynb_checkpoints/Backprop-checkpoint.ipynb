{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Backpropagation and Gradient Descent \n",
    "\n",
    "* Calculate the Loss (i.e. Error, Cost) at the end of the forward pass and then propagate the error backwards though the layers to the input.\n",
    "\n",
    "* Changing (i.e. updating) the weights is the only way to affect the loss. We use gradient descent to update the weights in neural networks.\n",
    "\n",
    "* Gradient descent: Minimize the loss function by iteratively moving in the direction of the negative of the gradient. \n",
    "\n",
    "* Training the network: Repeat propagating the error backwards and adjusting the weights until the loss is within some tolerance.\n",
    "\n",
    "* Three type of Gradient Descent\n",
    "    - Batch\n",
    "    - Stochastic\n",
    "    - Mini Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple Neural Network\n",
    "\n",
    "![](Simple_NN1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function: L = $\\frac{1}{2}(Y - y_{out})^2$  \n",
    "\n",
    "Derivative of Loss Function: $D(L) = -(Y - y_{out})$\n",
    "\n",
    "Sigmoid Function: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Derivative of Sigmoid: $D(\\sigma(z)) = \\sigma(z)(1 - \\sigma(z))$\n",
    "\n",
    "$$h_{in} = X \\cdot w_1,\\text{ } h_{out} = \\sigma(h_{in})$$\n",
    "\n",
    "$$y_{in} = h_{out} \\cdot w_2,\\text{ } y_{out} = \\sigma(y_{in})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the weights (Gradient Descent)\n",
    "\n",
    "* $\\gamma$ is the learning rate\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ w_1^{(t)} = w_1^{(t-1)} - \\gamma\\frac{\\partial{L}}{\\partial{w_1^{(t-1)}}}$$\n",
    "$$ w_2^{(t)} = w_2^{(t-1)} - \\gamma\\frac{\\partial{L}}{\\partial{w_2^{(t-1)}}}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial derivative of the Loss with respect to (wrt) the weights\n",
    "\n",
    "* Heavy use of chain rule\n",
    "\n",
    "* Weight for Output Layer\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$\\frac{\\partial{L}}{\\partial{w_2}} = \\frac{\\partial{L}}{\\partial{y_{out}}}\\frac{\\partial{y_{out}}}{\\partial{y_{in}}} \\frac{\\partial{y_{in}}}{\\partial{w_2}} = (Y - y_{out})(y_{out}(1 - y_{out}))h_{out}$$\n",
    "</div>\n",
    "\n",
    "* Weight for Hidden Layer\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$\\frac{\\partial{L}}{\\partial{w_1}} = \\frac{\\partial{L}}{\\partial{y_{out}}}\\frac{\\partial{y_{out}}}{\\partial{y_{in}}} \\frac{\\partial{y_{in}}}{\\partial{h_{out}}} \\frac{\\partial{h_{out}}}{\\partial{h_{in}}} \\frac{\\partial{h_{in}}}{\\partial{w_1}} =  (Y - y_{out})(y_{out}(1 - y_{out}))w_2(h_{out}(1 - h_{out}))X$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions and derivatives\n",
    "#   Sigmoid or Logistic function\n",
    "def Sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "# Derivative of Sigmoid or Logistic function\n",
    "def D_Sigmoid(z): \n",
    "    s = Sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "#\n",
    "# Loss Function\n",
    "def Loss(y,yhat):\n",
    "    return (1/2)*(y - yhat)**2\n",
    "\n",
    "def D_Loss(y, yhat):\n",
    "    return -(y - yhat)\n",
    "\n",
    "# Dot Product\n",
    "def Dot(x,w):\n",
    "    return np.dot(x,w)\n",
    "\n",
    "def D_Dot(x,w):\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Simple_NN1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Epoch(w_1,w_2):\n",
    "    \n",
    "    # Proprogate Forward\n",
    "    h_in = Dot(X,w_1)\n",
    "    h_out = Sigmoid(h_in)\n",
    "    y_in = Dot(h_out,w_2)\n",
    "    y_out = Sigmoid(y_in)\n",
    "    \n",
    "    # Backpropagation\n",
    "        \n",
    "    # Calculate the change in the error due to the weight in the second layer\n",
    "        # D_Loss(Y, y_out) =  -(Y - y_out)* \n",
    "        # D_Sigmoid(y_in) =  (y_out(1 - y_out))\n",
    "    delta_y = -(Y - y_out) * (y_out * (1 - y_out))\n",
    "        # D_Dot(h_out,w_2) = h_out\n",
    "    error_w2 = h_out.T.dot(delta_y) \n",
    "\n",
    "        # Calculate the change in the error due to the weight in weights in the first layer\n",
    "        # D_Dot(w_2, y_in) = w_2   \n",
    "        # D_Sigmoid(h_in) = h_out(1 - h_out) \n",
    "    delta_h = delta_y.dot(w_2.T) * (h_out * (1 - h_out))\n",
    "        # D_Dot(X,w_1) = X       \n",
    "    error_w1 = X.T.dot(delta_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w_2 = w_2 - gamma * error_w2\n",
    "    w_1 = w_1 - gamma * error_w1\n",
    "    return w_1,w_2, y_out        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One sample,  three features, single output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](NN1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[1.]] MSE: 0.039999999999999716  Epoch 2000\n"
     ]
    }
   ],
   "source": [
    "# Data: 1 trial, 3 features\n",
    "X = np.array([[.2, .4, .6]])\n",
    "Y = np.array([[.8]])\n",
    "\n",
    "# Weight Matricies\n",
    "w_1 = 2*np.random.random((3,1)) - 1\n",
    "w_2 = 2*np.random.random((1,1)) - 1\n",
    "\n",
    "Max_Epoch = 2000\n",
    "tolerance = 0.001 \n",
    "gamma = 80\n",
    "Converged = np.isclose(Y,0,tolerance)\n",
    "cnt = 0 \n",
    "while (not Converged.all()) and (cnt < Max_Epoch):\n",
    "    cnt += 1\n",
    "    w_1,w_2,y = One_Epoch(w_1,w_2)\n",
    "    # Check if Converged\n",
    "    Converged = np.isclose(Y,y,tolerance)\n",
    "    \n",
    "mse = np.mean((Y - y)**2) \n",
    "print(f\"Output: {y} MSE: {mse}  Epoch {cnt}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      " [[0.79344012]\n",
      " [0.19944846]\n",
      " [0.39776399]\n",
      " [0.49989846]] \n",
      " \n",
      " MSE: 0.002517163654541137  Epoch 40000\n"
     ]
    }
   ],
   "source": [
    "# 4 trials, 3 features\n",
    "X = np.array([[.2, .4, .6],   \n",
    "              [.3, .3, .1],\n",
    "              [.5, .4, .7],\n",
    "              [.8, .4, .1]])\n",
    "N = X.shape[0]\n",
    "Y = np.array([[.8],[.2], [.4], [.6]])\n",
    "w_1 = 2*np.random.random((3,N)) - 1\n",
    "w_2 = 2*np.random.random((N,1)) - 1\n",
    "\n",
    "gamma = 50\n",
    "tolerance = 0.001\n",
    "Max_Epoch = 40000\n",
    "Converged = np.isclose(Y,0)\n",
    "cnt = 0 \n",
    "\n",
    "while (not Converged.all()) and (cnt < Max_Epoch):\n",
    "    cnt += 1\n",
    "    w_1,w_2,y = One_Epoch(w_1,w_2)\n",
    "    # Check if Converged\n",
    "    Converged = np.isclose(Y,y,tolerance)\n",
    "    \n",
    "mse = np.mean((Y - y)**2) \n",
    "print(f\"Output: \\n {y} \\n \\n MSE: {mse}  Epoch {cnt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials:  [0 2 3 1]\n",
      "Trial 0 \n",
      " Output: \n",
      " [[0.7993416]] \n",
      " \n",
      " MSE: 4.3349255792988845e-07  Epoch 4\n",
      "Trial 2 \n",
      " Output: \n",
      " [[0.40034163]] \n",
      " \n",
      " MSE: 1.1671316297930424e-07  Epoch 6\n",
      "Trial 3 \n",
      " Output: \n",
      " [[0.59943858]] \n",
      " \n",
      " MSE: 3.151914421341889e-07  Epoch 13\n",
      "Trial 1 \n",
      " Output: \n",
      " [[0.20018748]] \n",
      " \n",
      " MSE: 3.5149809727710995e-08  Epoch 11\n"
     ]
    }
   ],
   "source": [
    "trials = np.random.permutation(range(4))\n",
    "print(\"Trials: \", trials)\n",
    "\n",
    "Data = np.array([[.2, .4, .6],\n",
    "              [.3, .3, .1],\n",
    "              [.5, .4, .7],\n",
    "              [.8, .4, .1]])\n",
    "\n",
    "Labels = np.array([[.8],[.2], [.4], [.6]])\n",
    "\n",
    "w_1 = 2*np.random.random((3,1)) - 1\n",
    "w_2 = 2*np.random.random((1,1)) - 1\n",
    "\n",
    "for tr in trials:\n",
    "    X = Data[tr,:].reshape(1,-1)\n",
    "    Y = Labels[tr,:].reshape(1,1)\n",
    "    \n",
    "    gamma = 80\n",
    "    \n",
    "    tolerance = 0.001\n",
    "    Max_Epoch = 40000\n",
    "    Converged = np.isclose(Y,0)\n",
    "    cnt = 0 \n",
    "\n",
    "    while (not Converged.all()) and (cnt < Max_Epoch):\n",
    "        cnt += 1\n",
    "        w_1,w_2,y = One_Epoch(w_1,w_2)\n",
    "        # Check if Converged\n",
    "        Converged = np.isclose(Y,y,tolerance)\n",
    "    \n",
    "    mse = np.mean((Y - y)**2) \n",
    "    print(f\"Trial {tr} \\n Output: \\n {y} \\n \\n MSE: {mse}  Epoch {cnt}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "* Splits the training data into small batches of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "254px",
    "left": "997px",
    "right": "20px",
    "top": "120px",
    "width": "315px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
